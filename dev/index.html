<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · MCIntegration.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://numericaleft.github.io/MCIntegration.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>MCIntegration.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Quick-start"><span>Quick start</span></a></li><li><a class="tocitem" href="#One-dimensional-integral"><span>One-dimensional integral</span></a></li><li><a class="tocitem" href="#Multi-dimensional-integral"><span>Multi-dimensional integral</span></a></li><li><a class="tocitem" href="#Evaluate-Multiple-Integrands-Simultaneously"><span>Evaluate Multiple Integrands Simultaneously</span></a></li><li><a class="tocitem" href="#Measure-Histogram"><span>Measure Histogram</span></a></li><li class="toplevel"><a class="tocitem" href="#Algorithm"><span>Algorithm</span></a></li><li class="toplevel"><a class="tocitem" href="#Variables"><span>Variables</span></a></li><li class="toplevel"><a class="tocitem" href="#Parallelization"><span>Parallelization</span></a></li><li><a class="tocitem" href="#MPI"><span>MPI</span></a></li><li><a class="tocitem" href="#Multi-threading"><span>Multi-threading</span></a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="lib/montecarlo/">Main module</a></li><li><a class="tocitem" href="lib/vegasmc/">Markov-chain based Vegas algorithm</a></li><li><a class="tocitem" href="lib/vegas/">Vegas algorithm</a></li><li><a class="tocitem" href="lib/mcmc/">Markov-chain Monte Carlo</a></li><li><a class="tocitem" href="lib/distribution/">Random Variables</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/numericaleft/MCIntegration.jl/blob/master/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="MCIntegration"><a class="docs-heading-anchor" href="#MCIntegration">MCIntegration</a><a id="MCIntegration-1"></a><a class="docs-heading-anchor-permalink" href="#MCIntegration" title="Permalink"></a></h1><p>Robust and efficient Monte Carlo calculator for high-dimensional integral.</p><p><a href="https://numericalEFT.github.io/MCIntegration.jl/stable"><img src="https://img.shields.io/badge/docs-stable-blue.svg" alt="Stable"/></a> <a href="https://numericalEFT.github.io/MCIntegration.jl/dev"><img src="https://img.shields.io/badge/docs-dev-blue.svg" alt="Dev"/></a> <a href="https://github.com/numericalEFT/MCIntegration.jl/actions"><img src="https://github.com/numericalEFT/MCIntegration.jl/workflows/CI/badge.svg" alt="Build Status"/></a> <a href="https://codecov.io/gh/numericalEFT/MCIntegration.jl"><img src="https://codecov.io/gh/numericalEFT/MCIntegration.jl/branch/master/graph/badge.svg" alt="Coverage"/></a></p><p>MCIntegration.jl provides several Monte Carlo algorithms to calculate regular/singular integrals in finite or inifinite dimensions.  </p><h1 id="Quick-start"><a class="docs-heading-anchor" href="#Quick-start">Quick start</a><a id="Quick-start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-start" title="Permalink"></a></h1><p>The following examples demonstrate the basic usage of this package. </p><h2 id="One-dimensional-integral"><a class="docs-heading-anchor" href="#One-dimensional-integral">One-dimensional integral</a><a id="One-dimensional-integral-1"></a><a class="docs-heading-anchor-permalink" href="#One-dimensional-integral" title="Permalink"></a></h2><p>We first show an example of highly singular integral. The following command evaluates ∫_0^1 log(x)/√x dx = 4.</p><pre><code class="language-julia hljs">julia&gt; res = integrate((x, c)-&gt;log(x[1])/sqrt(x[1]), solver=:vegas) 
Integral 1 = -3.997980772652019 ± 0.0013607691354676158   (chi2/dof = 1.93)

julia&gt; report(res) #print out the iteration history
====================================     Integral 1    ==========================================
  iter              integral                            wgt average                      chi2/dof
-------------------------------------------------------------------------------------------------
ignore        -3.8394711 ± 0.12101621              -3.8394711 ± 0.12101621                 0.0000
     2         -3.889894 ± 0.04161423              -3.8394711 ± 0.12101621                 0.0000
     3        -4.0258398 ± 0.016628525              -4.007122 ± 0.015441393                9.2027
     4        -4.0010193 ± 0.0097242712            -4.0027523 ± 0.0082285382               4.6573
     5         -3.990754 ± 0.0055248673            -3.9944823 ± 0.0045868638               3.5933
     6         -4.000744 ± 0.0025751679            -3.9992433 ± 0.0022454867               3.0492
     7        -4.0021542 ± 0.005940518             -3.9996072 ± 0.0021004392               2.4814
     8        -3.9979708 ± 0.0034603885            -3.9991666 ± 0.0017955468               2.0951
     9         -3.994137 ± 0.0026675679            -3.9975984 ± 0.0014895459               2.1453
    10        -3.9999099 ± 0.0033455927            -3.9979808 ± 0.0013607691               1.9269
-------------------------------------------------------------------------------------------------</code></pre><ul><li><p>By default, the function performs 10 iterations and each iteraction costs about <code>1e4</code> evaluations. You may reset these values with <code>niter</code> and <code>neval</code> keywords arguments.</p></li><li><p>The final result is obtained by inverse-variance-weighted averge of all the iterations except the first one (there is no important sampling yet!). They are stored in the return value<code>res</code>, which is a struct <a href="https://numericaleft.github.io/MCIntegration.jl/dev/lib/montecarlo/#Main-module"><code>Result</code></a>. You can access the statistics with <code>res.mean</code>, <code>res.stdev</code>, <code>res.chi2</code>, <code>res.dof</code> and <code>res.iterations</code> for all the iterations. </p></li><li><p>If you want to exclude more iterations, say the first three iterations, you can get a new result with the call <code>Result(res, 3)</code>.  </p></li><li><p>Internally, the <code>integrate</code> function optimizes the important sampling after each iteration. The results generally improves with iteractions. As long as <code>neval</code> is sufficiently large, the estimations from different iteractions should be statistically independent. This will justify an average of different iterations weighted by the inverse variance. The assumption of statically independence can be explicitly checked with chi-square test, namely <code>chi2/dof</code> should be about one. </p></li><li><p>You can pass the keyword arguemnt <code>solver</code> to the <code>integrate</code> functoin to specify the Monte Carlo algorithm. The above examples uses the Vegas algorithm with <code>:vegas</code>. In addition, this package provides two Markov-chain Monte Carlo algorithms for numerical integration. You can call them with <code>:vegasmc</code> or <code>:mcmc</code>. Check the Algorithm section for more details. </p></li><li><p>For the <code>:vegas</code> and <code>vegasmc</code> algorithms, the user-defined integrand evaluation function requires two arguments <code>(x, c)</code>, where <code>x</code> is the integration variable, while <code>c</code> is a struct stores the MC configuration. The latter contains additional information which may be needed for integrand evalution.  </p></li><li><p>The <a href="https://numericaleft.github.io/MCIntegration.jl/dev/lib/montecarlo/#Main-module">&#39;Configuration&#39;</a> struct stores the essential state information for the Monte Carlo sampling. Two particularly relavent members are</p><ul><li><code>userdata</code> : if you pass a keyword argument <code>userdata</code> to the <code>integrate</code> function, then it will be stored here, so that you can access it in your integrand evaluation function. </li><li><code>var</code> : A tuple of variables. In the above example, <code>var = (x, )</code> so that <code>var[1] === x</code>. </li></ul></li><li><p>The result returned by the <code>integrate</code> function contains the configuration after integration. If you want a detailed report, call <code>report(res.config)</code>. This configuration stores the optimized random variable distributions for the important sampling, which could be useful to evaluate other integrals with similar integrands. To use the optimized distributions, you can either call <code>integrate(..., config = res.config, ...)</code> to pass the entire configuration, or call <code>integrate(..., var = (res.config.var[1], ...), ...)</code> to pass one or more selected variables.</p></li></ul><h2 id="Multi-dimensional-integral"><a class="docs-heading-anchor" href="#Multi-dimensional-integral">Multi-dimensional integral</a><a id="Multi-dimensional-integral-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-dimensional-integral" title="Permalink"></a></h2><p>The following example first defines a pool of variables in [0, 1), then evaluates the area of a quarter unit circle (π/4 = 0.785398...).</p><pre><code class="language-julia hljs">julia&gt; X=Continuous(0.0, 1.0) #Create a pool of continuous variables. 
Adaptive continuous variable in the domain [0.0, 1.0). Max variable number = 16. Learning rate = 2.0.

julia&gt; integrate((X, c)-&gt;(X[1]^2+X[2]^2&lt;1.0); var = X, dof = 2)
Integral 1 = 0.7832652785953883 ± 0.002149843816733503   (chi2/dof = 1.28)</code></pre><h2 id="Evaluate-Multiple-Integrands-Simultaneously"><a class="docs-heading-anchor" href="#Evaluate-Multiple-Integrands-Simultaneously">Evaluate Multiple Integrands Simultaneously</a><a id="Evaluate-Multiple-Integrands-Simultaneously-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-Multiple-Integrands-Simultaneously" title="Permalink"></a></h2><p>You can calculate multiple integrals simultaneously. If the integrands are similar to each other, evaluating the integrals simultaneously sigificantly reduces cost. The following example calculate the area of a quarter circle and the volume of one-eighth sphere.</p><pre><code class="language-julia hljs">julia&gt; integrate((X, c)-&gt;(X[1]^2+X[2]^2&lt;1.0, X[1]^2+X[2]^2+X[3]^2&lt;1.0); var = Continuous(0.0, 1.0), dof = [[2,],[3,]])
Integral 1 = 0.7823432452235586 ± 0.003174967010742156   (chi2/dof = 2.82)
Integral 2 = 0.5185515421806122 ± 0.003219487569949905   (chi2/dof = 1.41)</code></pre><p>Here <code>dof</code> defines how many (degrees of freedom) variables of each type. For example, [[n1, n2], [m1, m2], ...] means the first integral involves n1 varibales of type 1, and n2 variables of type2, while the second integral involves m1 variables of type 1 and m2 variables of type 2. The <code>dof</code> of the integrals can be quite different, the program will figure out how to optimally padding the integrands to match the degrees of freedom. </p><p>You can also use the julia do-syntax to improve the readability of the above example,</p><pre><code class="language-julia hljs">julia&gt; integrate(var = Continuous(0.0, 1.0), dof = [[2,], [3,]]) do X, c
           r1 = (X[1]^2 + X[2]^2 &lt; 1.0) ? 1.0 : 0.0
           r2 = (X[1]^2 + X[2]^2 + X[3]^2 &lt; 1.0) ? 1.0 : 0.0
           return (r1, r2)
       end</code></pre><p>If there are too many components of integrands, it is better to preallocate the integrand weights. The function <code>integrate</code> provide an <code>inplace</code> key argument to achieve this goal. It is turned off by default, and only applies to the solver <code>:vegas</code> and <code>:vegasmc</code>. Once <code>inplace</code> is turned on, <code>integrate</code> will call the user-defined integrand function with a preallocated vector to store the user calculated weights. The following example demonstrates its usage,  </p><pre><code class="language-julia hljs">julia&gt; integrate(var = Continuous(0.0, 1.0), dof = [[2,], [3,]], inplace=true) do X, f, c
           f[1] = (X[1]^2 + X[2]^2 &lt; 1.0) ? 1.0 : 0.0
           f[2] = (X[1]^2 + X[2]^2 + X[3]^2 &lt; 1.0) ? 1.0 : 0.0
       end</code></pre><h2 id="Measure-Histogram"><a class="docs-heading-anchor" href="#Measure-Histogram">Measure Histogram</a><a id="Measure-Histogram-1"></a><a class="docs-heading-anchor-permalink" href="#Measure-Histogram" title="Permalink"></a></h2><p>You may want to study how an integral changes with a tuning parameter. The following example is how to solve the histogram measurement problem.</p><pre><code class="language-julia hljs">julia&gt; N = 20;

julia&gt; grid = [i / N for i in 1:N];

julia&gt; function integrand(vars, config)
            grid = config.userdata # radius
            x, bin = vars #unpack the variables
            r = grid[bin[1]] # binned variable in [0, 1)
            r1 = x[1]^2 + r^2 &lt; 1 # circle
            r2 = x[1]^2 + x[2]^2 + r^2 &lt; 1 # sphere
            return r1, r2
        end;

julia&gt; function measure(vars, obs, weights, config) 
       # obs: prototype of the observables for each integral
           x, bin = vars #unpack the variables
           obs[1][bin[1]] += weights[1] # circle
           obs[2][bin[1]] += weights[2] # sphere
       end;

julia&gt; res = integrate(integrand;
                measure = measure, # measurement function
                var = (Continuous(0.0, 1.0), Discrete(1, N)), # a continuous and a discrete variable pool 
                dof = [[1,1], [2,1]], 
                # integral-1: one continuous and one discrete variables, integral-2: two continous and one discrete variables
                obs = [zeros(N), zeros(N)], # prototype of the observables for each integral
                userdata = grid, neval = 1e5)
Integral 1 = 0.9957805541613277 ± 0.008336657854575344   (chi2/dof = 1.15)
Integral 2 = 0.7768105610812656 ± 0.006119386106596811   (chi2/dof = 1.4)</code></pre><p>You can visualize the returned result <code>res</code> with <code>Plots.jl</code>. The commands <code>res.mean[i]</code> and <code>res.stdev[i]</code> give the mean and stdev of the histogram of the <code>i</code>-th integral.</p><pre><code class="language-julia hljs">julia&gt; using Plots

julia&gt; plt = plot(grid, res.mean[1], yerror = res.stdev[1], xlabel=&quot;R&quot;, label=&quot;circle&quot;, aspect_ratio=1.0, xlim=[0.0, 1.0])

julia&gt; plot!(plt, grid, res.mean[2], yerror = res.stdev[2], label=&quot;sphere&quot;)</code></pre><p><img src="assets/circle_sphere.png?raw=true &quot;Circle and Sphere&quot;" alt="histogram"/></p><h1 id="Algorithm"><a class="docs-heading-anchor" href="#Algorithm">Algorithm</a><a id="Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithm" title="Permalink"></a></h1><p>This package provides three solvers.</p><ul><li>Vegas algorithm (<code>:vegas</code>): A Monte Carlo algorithm that uses importance sampling as a variance-reduction technique. Vegas iteratively builds up a piecewise constant weight function, represented</li></ul><p>on a rectangular grid. Each iteration consists of a sampling step followed by a refinement of the grid. The exact details of the algorithm can be found in <strong><em>G.P. Lepage, J. Comp. Phys. 27 (1978) 192, 3</em></strong> and <strong><em>G.P. Lepage, Report CLNS-80/447, Cornell Univ., Ithaca, N.Y., 1980</em></strong>. </p><ul><li><p>Vegas algorithm based on Markov-chain Monte Carlo (<code>:vegasmc</code>): A markov-chain Monte Carlo algorithm that uses the Vegas variance-reduction technique. It is as accurate as the vanilla Vegas algorithm, meanwhile tends to be more robust. For complicated high-dimensional integral, the vanilla Vegas algorithm can fail to learn the piecewise constant weight function. This algorithm uses Metropolis–Hastings algorithm to sample the integrand and improves the weight function learning.</p></li><li><p>Markov-chain Monte Carlo (<code>:mcmc</code>): This algorithm is useful for calculating bundled integrands that are too many to calculate at once. Examples are the path-integral of world lines of quantum particles, which involves hundreds and thousands of nested spacetime integrals. This algorithm uses the Metropolis-Hastings algorithm to jump between different integrals so that you only need to evaluate one integrand at each Monte Carlo step. Just as <code>:vegas</code> and <code>:vegasmc</code>, this algorithm also learns a piecewise constant weight function to reduce the variance. However, because it assumes you can access one integrand at each step, it tends to be less accurate than the other two algorithms for low-dimensional integrals.   </p></li></ul><p>The signature of the integrand and measure functions of the <code>:mcmc</code> solver receices an additional index argument than that of the <code>:vegas</code> and <code>:vegasmc</code> solvers. As shown in the above examples, the integrand and measure functions of the latter two solvers should be like <code>integrand(vars, config)</code> and <code>measure(vars, obs, weights, config)</code>, where <code>weights</code> is a vectors carries the values of the integrands at the current MC step. On the other hand, the <code>:mcmc</code> solver requires something like <code>integrand(idx, vars, config)</code> and <code>measure(idx, vars, weight, config)</code>, where <code>idx</code> is the index of the integrand of the current step, and the argument <code>weight</code> is a scalar carries the value of the current integrand being sampled.</p><h1 id="Variables"><a class="docs-heading-anchor" href="#Variables">Variables</a><a id="Variables-1"></a><a class="docs-heading-anchor-permalink" href="#Variables" title="Permalink"></a></h1><p>The package supports a couple of common types random variables. You can create them using the following constructors,</p><ul><li><code>Continous(lower, upper[; adapt = true, alpha = 3.0, ...])</code>: Continuous real-valued variables on the domain [lower, upper). MC will learn the distribution using the Vegas algorithm and then perform an imporant sampling accordingly.</li><li><code>Discrete(lower::Int, upper::Int[; adapt = true, alpha = 3.0, ...])</code>: Integer variables in the closed set [lower, upper]. MC will learn the distribution and perform an imporant sampling accordingly.</li></ul><p>After each iteration, the code will try to optimize how the variables are sampled, so that the most important regimes of the integrals will be sampled most frequently. Setting <code>alpha</code> to be true/false will turn on/off this distribution learning. The parameter <code>alpha</code> controls the learning rate.</p><p>When you call the above constructor, it creates an unlimited pool of random variables of a given type. The size of the pool will be dynamically determined when you call a solver. All variables in this pool will be sampled with the same distribution. In many high-dimensional integrals, many integration variables may contribute to the integral in a similar way; then they can be sampled from the same variable pool. For example, in the above code example, the integral for the circle area and the sphere volume both involve the variable type <code>Continuous</code>. The former has dof=2, while the latter has dof=3. To evaluate a given integrand, you only need to choose some of the variables to evaluate a given integral. The rest of the variables in the pool serve as dummy ones. They will not cause any computational overhead.</p><p>The variable pool trick will significantly reduce the cost of learning their distribution. It also opens the possibility of calculating integrals with infinite dimensions (for example, the path-integral of particle worldlines in quantum many-body physics). </p><p>If some of the variables are paired with each other (for example, the three continuous variables (r, θ, ϕ) representing a 3D vector), then you can pack them into a joint random variable, which can be constructed with the following constructor,</p><ul><li><code>CompositeVar(var1, var2, ...[; adapt = true, alpha = 3.0, ...])</code>: A product of different types of random variables. It samples <code>var1</code>, <code>var2</code>, ... with their producted distribution. </li></ul><p>The packed variables will be sampled all together in the Markov-chain based solvers (<code>:vegasmc</code> and <code>:mcmc</code>). Such updates will generate more independent samples compared to the unpacked version. Sometimes, it could reduce the auto-correlation time of the Markov chain and make the algorithm more efficient.</p><p>Moreover, packed variables usually indicate nontrivial correlations between their distributions. In the future, it will be interesting to learn such correlation so that one can sample the packed variables more efficiently.</p><div class="admonition is-success"><header class="admonition-header">Integrate over different domains</header><div class="admonition-body"><p>If you want to compute an integral over a semi-infinite or an inifite domain, you can follow <a href="http://ab-initio.mit.edu/wiki/index.php/Cubature#Infinite_intervals">this advice</a> from Steven G. Johnson: to compute an integral over a semi-infinite interval, you can perform the change of variables <span>$x=a+y/(1-y)$</span>:</p><p class="math-container">\[\int_{a}^{\infty} f(x)\,\mathrm{d}x = \int_{0}^{1}
f\left(a + \frac{y}{1 - y}\right)\frac{1}{(1 - y)^2}\,\mathrm{d}y\]</p><p>For an infinite interval, you can perform the change of variables <span>$x=(2y - 1)/((1 - y)y)$</span>:</p><p class="math-container">\[\int_{-\infty}^{\infty} f(x)\,\mathrm{d}x = \int_{0}^{1}
f\left(\frac{2y - 1}{(1 - y)y}\right)\frac{2y^2 - 2y + 1}{(1 -
y)^2y^2}\,\mathrm{d}y\]</p><p>In addition, recall that for an <a href="https://en.wikipedia.org/wiki/Even_and_odd_functions#Even_functions">even function</a> <span>$\int_{-\infty}^{\infty} f(x)\,\mathrm{d}x = 2\int_{0}^{\infty}f(x)\,\mathrm{d}x$</span>,  while the integral of an <a href="https://en.wikipedia.org/wiki/Even_and_odd_functions#Odd_functions">odd function</a> over the infinite interval <span>$(-\infty, \infty)$</span> is zero.</p></div></div><h1 id="Parallelization"><a class="docs-heading-anchor" href="#Parallelization">Parallelization</a><a id="Parallelization-1"></a><a class="docs-heading-anchor-permalink" href="#Parallelization" title="Permalink"></a></h1><p>MCIntegration supports both MPI and multi-thread parallelization. You can even mix them if necessary.</p><h2 id="MPI"><a class="docs-heading-anchor" href="#MPI">MPI</a><a id="MPI-1"></a><a class="docs-heading-anchor-permalink" href="#MPI" title="Permalink"></a></h2><p>To run your code in MPI mode, simply use the command,</p><pre><code class="language-bash hljs">mpiexec -n #NCPU julia ./your_script.jl</code></pre><p>where <code>#NCPU</code> is the number of workers. Internally, the MC sampler will send the blocks (controlled by the argument <code>Nblock</code>, see above example code) to different workers, then collect the estimates in the root node. </p><p>Note that you need to install the package <a href="https://github.com/JuliaParallel/MPI.jl">MPI.jl</a> to use the MPI mode. See this <a href="https://juliaparallel.github.io/MPI.jl/stable/configuration/">link</a> for the instruction on the configuration.</p><p>The user essentially doesn&#39;t need to write additional code to support the parallelization. The only tricky part is the output: only the function <code>MCIntegratoin.integrate</code> of the root node returns meaningful estimates, while other workers simply returns <code>nothing</code>.</p><h2 id="Multi-threading"><a class="docs-heading-anchor" href="#Multi-threading">Multi-threading</a><a id="Multi-threading-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-threading" title="Permalink"></a></h2><p>MCIntegration supports multi-threading with or without MPI. To run your code with multiple threads, start Julia with</p><pre><code class="language-bash hljs">julia -t #NCPU ./your_script.jl</code></pre><p>Note that all threads will share the same memory. The user-defined <code>integrand</code> and <code>measure</code> functions should be implemented thread-safe (for example, be very careful about reading any data if another thread might write to it). We recommend the user read Julia&#39;s official <a href="https://docs.julialang.org/en/v1/manual/multi-threading/">documentation</a>.</p><p>There are two different ways to parallelize your code with multiple threads. </p><ol><li>If you need to evaluate multiple integrals, each thread can call the function <code>MCIntegration.integrate</code> to do one integral. In the following example, we use three threads to evaluate three integrals altogether. Note that only three threads will be used even if you initialize Julia with more than three threads.</li></ol><pre><code class="language-julia hljs">julia&gt; Threads.@threads for i = 1:3
       println(&quot;Thread $(Threads.threadid()) returns &quot;, integrate((x, c) -&gt; x[1]^i, print=-2))
       end
Thread 2 returns Integral 1 = 0.24995156136254149 ± 6.945088534643841e-5   (chi2/dof = 2.95)
Thread 3 returns Integral 1 = 0.3334287563137184 ± 9.452648803649706e-5   (chi2/dof = 1.35)
Thread 1 returns Integral 1 = 0.5000251243601586 ± 0.00013482206569391864   (chi2/dof = 1.58)</code></pre><ol><li>Only the main thread calls the function <code>MCIntegration.integrate</code>, then parallelize the internal blocks with multiple threads. To do that, you need to call the function <code>MCIntegration.integrate</code> with a key argument <code>parallel = :thread</code>. This approach will utilize all Julia threads.  For example,</li></ol><pre><code class="language-julia hljs">julia&gt; for i = 1:3
       println(&quot;Thread $(Threads.threadid()) return &quot;, integrate((x, c) -&gt; x[1]^i, print=-2, parallel=:thread))
       end
Thread 1 return Integral 1 = 0.5001880440214347 ± 0.00015058935731086765   (chi2/dof = 0.397)
Thread 1 return Integral 1 = 0.33341068551139696 ± 0.00010109649819894601   (chi2/dof = 1.94)
Thread 1 return Integral 1 = 0.24983868976137244 ± 8.546009018501706e-5   (chi2/dof = 1.54)</code></pre></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="lib/montecarlo/">Main module »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.5 on <span class="colophon-date" title="Thursday 23 February 2023 16:51">Thursday 23 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
