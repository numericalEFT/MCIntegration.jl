var documenterSearchIndex = {"docs":
[{"location":"lib/vegas/#Vegas-algorithm","page":"Vegas algorithm","title":"Vegas algorithm","text":"","category":"section"},{"location":"lib/vegas/","page":"Vegas algorithm","title":"Vegas algorithm","text":"Modules = [MCIntegration.Vegas]","category":"page"},{"location":"lib/vegas/#MCIntegration.Vegas.montecarlo-Union{Tuple{T}, Tuple{O}, Tuple{P}, Tuple{V}, Tuple{Ni}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any, Any, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any, Any, Any, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any, Any, Any, Any, Any}} where {Ni, V, P, O, T}","page":"Vegas algorithm","title":"MCIntegration.Vegas.montecarlo","text":"function montecarlo(config::Configuration{Ni,V,P,O,T}, integrand::Function, neval,\n    print=0, save=0, timer=[], debug=false;\n    measure::Union{Nothing,Function}=nothing, measurefreq=1, kwargs...) where {Ni,V,P,O,T}\n\nThis algorithm implements the classic Vegas algorithm.\n\nThe main idea of the algorithm can be found in this link.\n\nThe algorithm uses a simple important sampling scheme.  Consider an one-dimensional integral with the integrand f(x), the algorithm will try to learn an optimized distribution rho(x)0 which mimic the integrand as good as possible (a.k.a, the Vegas map trick, see Dist.Continuous) for more detail. \n\nOne then sample the variable x with the distribution rho(x), and estimate the integral by averging the estimator f(x)rho(x).\n\nNOTE: If there are more than one integrals, then all integrals are sampled and measured at each Monte Carlo step.\n\nThis algorithm is very efficient for low-dimensional integrations, but can be less efficient and less robust than the Markov-chain Monte Carlo solvers for high-dimensional integrations.\n\nArguments\n\nintegrand : User-defined function with the following signature:\n\nfunction integrand(var, config)\n    # calculate your integrand values\n    # return integrand1, integrand2, ...\nend\n\nThe first parameter var is either a Variable struct if there is only one type of variable, or a tuple of Varibles if there are more than one types of variables. The second parameter passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\n\nmeasure : User-defined function with the following signature:\n\nfunction measure(var, obs, weights, config)\n    # accumulates the weight into the observable\n    # For example,\n    # obs[1] = weights[1] # integral 1\n    # obs[2] = weights[2] # integral 2\n    # ...\nend\n\nThe first argument var is either a Variable struct if there is only one type of variable, or a tuple of Varibles if there are more than one types of variables. The second argument passes the user-defined observable to the function, it should be a vector with the length same as the integral number. The third argument is the integrand weights to be accumulated to the observable, it is a vector with the length same as the integral number. The last argument passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\n\nExamples\n\nThe following command calls the Vegas solver,\n\njulia> integrate((x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = 2, print=-1, solver=:vegas)\nIntegral 1 = 0.667203631824444 ± 0.0005046485925614018   (chi2/dof = 1.46)\n\n\n\n\n\n","category":"method"},{"location":"lib/vegasmc/#Markov-chain-based-Vegas-algorithm","page":"Markov-chain based Vegas algorithm","title":"Markov-chain based Vegas algorithm","text":"","category":"section"},{"location":"lib/vegasmc/","page":"Markov-chain based Vegas algorithm","title":"Markov-chain based Vegas algorithm","text":"Modules = [MCIntegration.VegasMC]","category":"page"},{"location":"lib/vegasmc/#MCIntegration.VegasMC.montecarlo-Union{Tuple{T}, Tuple{O}, Tuple{P}, Tuple{V}, Tuple{N}, Tuple{Configuration{N, V, P, O, T}, Function, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any, Any, Any}} where {N, V, P, O, T}","page":"Markov-chain based Vegas algorithm","title":"MCIntegration.VegasMC.montecarlo","text":"function montecarlo(config::Configuration{N,V,P,O,T}, integrand::Function, neval,\n    print=0, debug=false;\n    measurefreq=2, measure::Union{Nothing,Function}=nothing,\n    kwargs...) where {N,V,P,O,T}\n\nThis algorithm combines Vegas with Markov-chain Monte Carlo. For multiple integrands invoves multiple variables, it finds the best distribution ansatz to fit them all together. In additional to the original integral, it also  introduces a normalization integral with integrand ~ 1.\n\nAssume we want to calculate the integral f_1(x) and f_2(x y), where x, y are two different types of variables. The algorithm will try to learn a distribution rho_x(x) and rho_y(y) so that f_1(x)rho_x(x) and f_2(x y)rho_x(x)rho_y(y) are as flat as possible. \n\nThe algorithm then samples the variables x and y with a joint distribution using the Metropolis-Hastings algorithm, math p(x y) = r_0 cdot rho_x(x) cdot rho_y(y) + r_1 cdot f_1(x) cdot rho_y(y) + r_2 cdot f_2(x y) where r_i are certain reweighting factor to make sure all terms contribute same weights. One then estimate the integrals by averaging the observables f_1(x)rho_y(y)p(x y) and f_2(x y)p(x y).\n\nThis algorithm reduces to the vanilla Vegas algorithm by setting r_0 = 1 and r_i0 = 0.\n\nNOTE: If there are more than one integrals,  all of them are sampled and measured at each Markov-chain Monte Carlo step!\n\nThis algorithm is as efficient as the Vegas algorithm for low-dimensional integration, and  tends to be more robust than the Vegas algorithm for high-dimensional integration. \n\nArguments\n\nintegrand : User-defined function with the following signature:\n\nfunction integrand(var, config)\n    # calculate your integrand values\n    # return integrand1, integrand2, ...\nend\n\nThe first parameter var is either a Variable struct if there is only one type of variable, or a tuple of Varibles if there are more than one types of variables. The second parameter passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\n\nmeasure : User-defined function with the following signature:\n\nfunction measure(var, obs, weights, config)\n    # accumulates the weight into the observable\n    # For example,\n    # obs[1] = weights[1] # integral 1\n    # obs[2] = weights[2] # integral 2\n    # ...\nend\n\nThe first argument var is either a Variable struct if there is only one type of variable, or a tuple of Varibles if there are more than one types of variables. The second argument passes the user-defined observable to the function, it should be a vector with the length same as the integral number. The third argument is the integrand weights to be accumulated to the observable, it is a vector with the length same as the integral number. The last argument passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\n\nExamples\n\nThe following command calls the MC Vegas solver,\n\njulia> integrate((x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = 2, print=-1, solver=:vegasmc)\nIntegral 1 = 0.6640840471808533 ± 0.000916060916265263   (chi2/dof = 0.945)\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#Random-Variables","page":"Random Variables","title":"Random Variables","text":"","category":"section"},{"location":"lib/distribution/","page":"Random Variables","title":"Random Variables","text":"Modules = [MCIntegration.Dist]","category":"page"},{"location":"lib/distribution/#MCIntegration.Dist.CompositeVar-Tuple","page":"Random Variables","title":"MCIntegration.Dist.CompositeVar","text":"function CompositeVar(vargs...; adapt=true)\n\nCreate a product of different types of random variables. The bundled variables will be sampled with their producted distribution.\n\nArguments:\n\nvargs  : tuple of Variables\nadapt  : turn on or off the adaptive map\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.Continuous-Union{Tuple{Float64, Float64}, Tuple{G}, Tuple{Float64, Float64, Any}} where G","page":"Random Variables","title":"MCIntegration.Dist.Continuous","text":"function Continuous(lower::Float64, upper::Float64; ninc = 1000, alpha=2.0, adapt=true) where {G}\n\nCreate a pool of continous variables sampling from the set [lower, upper) with a distribution generated by a Vegas map (see below).  The distribution is trained after each iteraction if adapt = true.\n\nArguments:\n\nlower  : lower bound\nupper  : upper bound\nninc   : number of increments\nalpha  : learning rate\nadapt  : turn on or off the adaptive map\n\nRemark:\n\nVegas map maps the original integration variables x into new variables y, so that the integrand is as flat as possible in y:\n\nbeginaligned\nx_0 = a \nx_1 = x_0 + Delta x_0 \nx_2 = x_1 + Delta x_1 \ncdots \nx_N = x_N-1 + Delta x_N-1 = b\nendaligned\n\nwhere a and b are the limits of integration. The grid specifies the transformation function at the points y=iN for i=01ldots N:\n\nx(y=iN) = x_i\n\nLinear interpolation is used between those points. The Jacobian for this transformation is:\n\nJ(y) = J_i = N Delta x_i\n\nThe grid point x_i is trained after each iteration.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.Discrete","page":"Random Variables","title":"MCIntegration.Dist.Discrete","text":"function Discrete(lower::Int, upper::Int; distribution=nothing, alpha=2.0, adapt=true)\n\nCreate a pool of integer variables sampling from the closed set [lower, lower+1, ..., upper] with the distribution Discrete.distribution.  The distribution is trained after each iteraction ifadapt = true`.\n\nArguments:\n\nlower  : lower bound\nupper  : upper bound\ndistributin   : inital distribution \nalpha  : learning rate\nadapt  : turn on or off the adaptive map\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.Variable","page":"Random Variables","title":"MCIntegration.Dist.Variable","text":"abstract type Variable end\n\nAbstract Type of all variable pools. \n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.accumulate!-Tuple{MCIntegration.Dist.Variable, Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.accumulate!","text":"accumulate!(var::Variable, idx, weight) = nothing\n\nAccumulate a new sample with the a given weight for the idx-th element of the Variable pool var.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.clearStatistics!-Tuple{MCIntegration.Dist.Variable}","page":"Random Variables","title":"MCIntegration.Dist.clearStatistics!","text":"clearStatistics!(T::Variable)\n\nClear the accumulated samples in the Variable.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.initialize!-Tuple{MCIntegration.Dist.Variable, Any}","page":"Random Variables","title":"MCIntegration.Dist.initialize!","text":"initialize!(T::Variable, config)\n\nInitialize the variable pool with random variables.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.locate-Tuple{AbstractVector, Number}","page":"Random Variables","title":"MCIntegration.Dist.locate","text":"function locate(accumulation, p)\n\nReturn index of p in accumulation so that accumulation[idx]<=p<accumulation[idx+1].  If p is not in accumulation (namely accumulation[1] > p or accumulation[end] <= p), return -1. Bisection algorithmn is used so that the time complexity is O(log(n)) with n=length(accumulation).\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.padding_probability-Tuple{Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.padding_probability","text":"padding_probability(config, idx)\n\nCalculate the joint probability of missing variables for the idx-th integral compared to the full variable set.\n\npadding_probability(config, idx) = total_probability(config) / probability(config, idx)\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.probability-Tuple{Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.probability","text":"probability(config, idx)\n\nCalculate the joint probability of all involved variable for the idx-th integral.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.rescale","page":"Random Variables","title":"MCIntegration.Dist.rescale","text":"function rescale(dist::AbstractVector, alpha=1.5)\n\nRescale the dist array to avoid overreacting to atypically large number.\n\nThere are three steps:\n\ndist will be first normalize to [0, 1].\nThen the values that are close to 1.0 will not be changed much, while that close to zero will be amplified to a value controlled by alpha.\nIn the end, the rescaled dist array will be normalized to [0, 1].\n\nCheck Eq. (19) of https://arxiv.org/pdf/2009.05112.pdf for more detail\n\n\n\n\n\n","category":"function"},{"location":"lib/distribution/#MCIntegration.Dist.smooth","page":"Random Variables","title":"MCIntegration.Dist.smooth","text":"function smooth(dist::AbstractVector, factor=6)\n\nSmooth the distribution by averaging two nearest neighbor. The average ratio is given by 1 : factor : 1 for the elements which are not on the boundary.\n\n\n\n\n\n","category":"function"},{"location":"lib/distribution/#MCIntegration.Dist.total_probability-Tuple{Any}","page":"Random Variables","title":"MCIntegration.Dist.total_probability","text":"total_probability(config)\n\nCalculate the joint probability of all involved variables of all integrals.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.train!-Tuple{MCIntegration.Dist.Variable}","page":"Random Variables","title":"MCIntegration.Dist.train!","text":"train!(Var::Variable)\n\nTrain the distribution of the variables in the pool.\n\n\n\n\n\n","category":"method"},{"location":"lib/montecarlo/#Main-module","page":"Main module","title":"Main module","text":"","category":"section"},{"location":"lib/montecarlo/","page":"Main module","title":"Main module","text":"Modules = [MCIntegration]","category":"page"},{"location":"lib/montecarlo/#MCIntegration.Configuration","page":"Main module","title":"MCIntegration.Configuration","text":"mutable struct Configuration\n\nStruct that contains everything needed for MC.\n\nStatic parameters\n\nseed: seed to initialize random numebr generator, also serves as the unique pid of the configuration\nrng: a MersenneTwister random number generator, seeded by seed\nuserdata: user-defined parameter\nvar: TUPLE of variables, each variable should be derived from the abstract type Variable, see variable.jl for details). Use a tuple rather than a vector improves the performance.\n\nintegrand properties\n\nneighbor::Vector{Tuple{Int, Int}} : vector of tuples that defines the neighboring integrands. Two neighboring integrands are directly connected in the Markov chain.    e.g., [(1, 2), (2, 3)] means the integrand 1 and 2 are neighbor, and 2 and 3 are neighbor.    The neighbor vector defines a undirected graph showing how the integrands are connected. Please make sure all integrands are connected.  By default, we assume the N integrands are in the increase order, meaning the neighbor will be set to [(N+1, 1), (1, 2), (2, 4), ..., (N-1, N)], where the first N entries are for diagram 1, 2, ..., N and the last entry is for the normalization diagram. Only the first diagram is connected to the normalization diagram.  Only highly correlated integrands are not highly correlated should be defined as neighbors. Otherwise, most of the updates between the neighboring integrands will be rejected and wasted.\ndof::Vector{Vector{Int}}: degrees of freedom of each integrand, e.g., [[0, 1], [2, 3]] means the first integrand has zero var#1 and one var#2; while the second integrand has two var#1 and 3 var#2. \nobservable: observables that is required to calculate the integrands, will be used in the measure function call.   It is either an array of any type with the common operations like +-*/^ defined. \nreweight: reweight factors for each integrands. The reweight factor of the normalization diagram is assumed to be 1. Note that you don't need to explicitly add the normalization diagram. \nvisited: how many times this integrand is visited by the Markov chain.\n\ncurrent MC state\n\nstep: the number of MC updates performed up to now\nnorm: the index of the normalization diagram. norm is larger than the index of any user-defined integrands \nnormalization: the accumulated normalization factor. Physical observable = Configuration.observable/Configuration.normalization.\npropose/accept: array to store the proposed and accepted updates for each integrands and variables.  Their shapes are (number of updates X integrand number X max(integrand number, variable number).  The last index will waste some memory, but the dimension is small anyway.\n\n\n\n\n\n","category":"type"},{"location":"lib/montecarlo/#MCIntegration.Configuration-Tuple{}","page":"Main module","title":"MCIntegration.Configuration","text":"function Configuration(;\n    var::Union{Variable,AbstractVector,Tuple}=(Continuous(0.0, 1.0),),\n    dof::Union{Int,AbstractVector,AbstractMatrix}=[ones(Int, length(var)),],\n    type=Float64,  # type of the integrand\n    obs::AbstractVector=zeros(type, length(dof)),\n    reweight::Vector{Float64}=ones(length(dof) + 1),\n    seed::Int=rand(Random.RandomDevice(), 1:1000000),\n    neighbor::Union{Vector{Vector{Int}},Vector{Tuple{Int,Int}},Nothing}=nothing,\n    userdata=nothing,\n    kwargs...\n)\n\nCreate a Configuration struct\n\nArguments\n\nvar: TUPLE of variables, each variable should be derived from the abstract type Variable, see variable.jl for details). Use a tuple rather than a vector improves the performance.\n\nBy default, var = (Continuous(0.0, 1.0),), which is a single continuous variable.\n\ndof::Vector{Vector{Int}}: degrees of freedom of each integrand, e.g., [[0, 1], [2, 3]] means the first integrand has zero var#1 and one var#2; while the second integrand has two var#1 and 3 var#2. \n\nBy default, dof=[ones(length(var)), ], which means that there is only one integrand, and each variable has one degree of freedom.\n\nobs: observables that is required to calculate the integrands, will be used in the measure function call.\n\nIt is either an array of any type with the common operations like +-*/^ defined.  By default, it will be set to 0.0 if there is only one integrand (e.g., length(dof)==1); otherwise, it will be set to zeros(length(dof)).\n\npara: user-defined parameter, set to nothing if not needed\nreweight: reweight factors for each integrands. If not set, then all factors will be initialized with one.\nseed: seed to initialize random numebr generator, also serves as the unique pid of the configuration. If it is nothing, then use RandomDevice() to generate a random seed in [1, 1000_1000]\nneighbor::Vector{Tuple{Int, Int}} : vector of tuples that defines the neighboring integrands. Two neighboring integrands are directly connected in the Markov chain.    e.g., [(1, 2), (2, 3)] means the integrand 1 and 2 are neighbor, and 2 and 3 are neighbor.     The neighbor vector defines a undirected graph showing how the integrands are connected. Please make sure all integrands are connected.   By default, we assume the N integrands are in the increase order, meaning the neighbor will be set to [(N+1, 1), (1, 2), (2, 4), ..., (N-1, N)], where the first N entries are for diagram 1, 2, ..., N and the last entry is for the normalization diagram. Only the first diagram is connected to the normalization diagram.   Only highly correlated integrands are not highly correlated should be defined as neighbors. Otherwise, most of the updates between the neighboring integrands will be rejected and wasted.\nuserdata: User data you want to pass to the integrand and the measurement\n\n\n\n\n\n","category":"method"},{"location":"lib/montecarlo/#MCIntegration.Result","page":"Main module","title":"MCIntegration.Result","text":"struct Result{O,C}\n\nthe returned result of the MC integration.\n\nMembers\n\nmean: mean of the MC integration\nstdev: standard deviation of the MC integration\nchi2: chi-square per dof of the MC integration\nneval: number of evaluations of the integrand\nignore: ignore iterations untill ignore\ndof: degrees of freedom of the MC integration (number of iterations - 1)\nconfig: configuration of the MC integration from the last iteration\niterations: list of tuples [(data, error, Configuration), ...] from each iteration\n\n\n\n\n\n","category":"type"},{"location":"lib/montecarlo/#MCIntegration.average","page":"Main module","title":"MCIntegration.average","text":"function average(history, idx=1; init=1, max=length(history))\n\naverage the history[1:max]. Return the mean, standard deviation and chi2 of the history.\n\nArguments\n\nhistory: a list of tuples, such as [(data, error, Configuration), ...]\nidx: the index of the integral\nmax: the last index of the history to average with\ninit : the first index of the history to average with\n\n\n\n\n\n","category":"function"},{"location":"lib/montecarlo/#MCIntegration.integrate-Tuple{Function}","page":"Main module","title":"MCIntegration.integrate","text":"function integrate(integrand::Function;\n    config::Union{Configuration,Nothing}=nothing,\n    solver::Symbol=:vegas, # :mcmc, :vegas, or :vegasmc\n    neval=1e4, \n    niter=10, \n    block=16, \n    gamma=1.0, \n    print=-1, \n    debug=false, \n    reweight_goal::Union{Vector{Float64},Nothing}=nothing, \n    ignore::Int=adapt ? 1 : 0,\n    kwargs...\n)\n\nCalculate the integrals, collect statistics, and return a Result struct that contains the estimations and errors.\n\nRemarks\n\nUser may run the MC in parallel using MPI. Simply run mpiexec -n N julia userscript.jl where N is the number of workers. In this mode, only the root process returns meaningful results. All other workers return nothing, nothing. User is responsible to handle the returning results properly. If you have multiple number of mpi version, you can use \"mpiexecjl\" in your \"~/.julia/package/MPI/###/bin\" to make sure the version is correct. See https://juliaparallel.github.io/MPI.jl/stable/configuration/ for more detail.\nIn the MC, a normalization diagram is introduced to normalize the MC estimates of the integrands. More information can be found in the link: https://kunyuan.github.io/QuantumStatistics.jl/dev/man/important_sampling/#Important-Sampling. User don't need to explicitly specify this normalization diagram.Internally, normalization diagram will be added to each table that is related to the integrands.\n\nArguments\n\nintegrand:Function call to evaluate the integrand. It should accept an argument of the type Configuration, and return a weight.              Internally, MC only samples the absolute value of the weight. Therefore, it is also important to define Main.abs for the weight if its type is user-defined. \nsolver :  :vegas, :vegasmc, or :vegasmc. See Readme for more details.\nconfig:   Configuration object to perform the MC integration. If nothing, it attempts to create a new one with Configuration(; kwargs...).\nneval:    Number of evaluations of the integrand per iteration. \nniter:    Number of iterations. The reweight factor and the variables will be self-adapted after each iteration. \nblock:    Number of blocks. Each block will be evaluated by about neval/block times. Each block is assumed to be statistically independent, and will be used to estimate the error.              In MPI mode, the blocks are distributed among the workers. If the numebr of workers N is larger than block, then block will be set to be N.\ngamma:    Learning rate of the reweight factor after each iteraction. Note that alpha <=1, where alpha = 0 means no reweighting.  \nprint:    -1 to not print anything, 0 to print minimal information, >0 to print summary for every print seconds\ndebug:    Whether to print debug information (type instability, float overflow etc.)\nreweight_goal: The expected distribution of visited times for each integrand after reweighting . If not set, then all factors will be initialized with one. Only useful for the :mcmc solver. \nignore : ignore the iteration until the ignore round. By default, the first iteration is igonred if adapt=true, and non is ignored if adapt=false.\nkwargs:   Keyword arguments. If config is nothing, you may need to provide arguments for the Configuration constructor, check Configuration docs for more details.\n\nExamples\n\njulia> integrate((x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = 2, print=-1)\nIntegral 1 = 0.6640840471808533 ± 0.000916060916265263   (chi2/dof = 0.945)\n\n\n\n\n\n","category":"method"},{"location":"lib/montecarlo/#MCIntegration.report","page":"Main module","title":"MCIntegration.report","text":"function report(result::Result, ignore=result.ignore; pick::Union{Function,AbstractVector}=obs -> first(obs), name=nothing, verbose=0)\n\nprint the summary of the result.  It will first print the configuration from the last iteration, then print the weighted average and standard deviation of the picked observable from each iteration.\n\nArguments\n\nresult: Result object contains the history from each iteration\nignore: the ignore the first # iteractions.\npick: The pick function is used to select one of the observable to be printed. The return value of pick function must be a Number.\nname: name of each picked observable. If name is not given, the index of the pick function will be used.\n\n\n\n\n\n","category":"function"},{"location":"man/important_sampling/#Important-Sampling","page":"Important Sampling","title":"Important Sampling","text":"","category":"section"},{"location":"man/important_sampling/#Introduction","page":"Important Sampling","title":"Introduction","text":"","category":"section"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"This note compares two important sampling approaches for Monte Carlo integration. The first approach introduces a normalization sector and lets the Markov chain jumps between this additional sector and the integrand sector following a calibrated probability density for important sampling. One can infer the integration between the ratio of weights between two sectors. On the other hand, the second approach reweights the original integrand to make it as flat as possible, one then perform a random walk uniformly in the parameter space to calculate the integration. This is the conventional approach used in Vegas algorithm.","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"In general, the first approach is more robust than the second one, but less efficient. In many applications, for example, high order Feynman diagrams with a sign alternation, the important sampling probability can't represent the complicated integrand well. Then the first approach is as efficient as the second one, but tends to be much robust.","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"We next present a benchmark between two approaches. Consider the MC sampling of an one-dimensional functions f(x) (its sign may oscillate).","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"We want to design an efficient algorithm to calculate the integral int_a^b dx f(x). To do that, we normalize the integrand with an ansatz g(x)0 to reduce the variant. ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Our package supports two important sampling schemes. ","category":"page"},{"location":"man/important_sampling/#Approach-1:-Algorithm-with-a-Normalization-Sector","page":"Important Sampling","title":"Approach 1: Algorithm with a Normalization Sector","text":"","category":"section"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"In this approach, the configuration spaces consist of two sub-spaces: the physical sector with orders nge 1 and the normalization sector with the order n=0. The weight function of the latter, g(x), should be simple enough so that the integral G=int g(x) d x is explicitly known. In our algorithm we use a constant g(x) propto 1 for simplicity. In this setup, the physical sector weight, namely the integral F = int f(x) dx, can be calculated with the equation","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"    F=fracF_rm MCG_rm MC G","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"where the MC estimators F_rm MC and G_rm MC are measured with ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"F_rm MC =frac1N left sum_i=1^N_f fracf(x_i)rho_f(x_i) + sum_i=1^N_g 0 right","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"and","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"G_rm MC =frac1N leftsum_i=1^N_f 0 + sum_i=1^N_g fracg(x_i)rho_g(x_i)  right","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"The probability density of a given configuration is proportional to rho_f(x)=f(x) and rho_g(x)=g(x), respectively. After N MC updates, the physical sector is sampled for N_f times, and the normalization sector is for N_g times. ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Now we estimate the statistic error. According to the propagation of uncertainty, the variance of F  is given by","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":" fracsigma^2_FF^2 =  fracsigma_F_rm MC^2F_MC^2 + fracsigma_G_rm MC^2G_MC^2 ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"where sigma_F_rm MC and sigma_G_rm MC are variance of the MC integration F_rm MC and G_rm MC, respectively. In the Markov chain MC, the variance of F_rm MC can be written as ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"sigma^2_F_rm MC = frac1N left sum_i^N_f left( fracf(x_i)rho_f(x_i)- fracFZright)^2 +sum_j^N_g left(0-fracFZ right)^2  right ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"= int left( fracf(x)rho_f(x) - fracFZ right)^2 fracrho_f(x)Z rm dx + int left( fracFZ right)^2 fracrho_g(x)Z dx ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"=  int fracf^2(x)rho_f(x) fracdxZ -fracF^2Z^2 ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Here Z=Z_f+Z_g and Z_fg=int rho_fg(x)dx are the partition sums of the corresponding configuration spaces. Due to the detailed balance, one has Z_fZ_g=N_fN_g.  ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Similarly, the variance of G_rm MC can be written as ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"sigma^2_G_rm MC=  int fracg^2(x)rho_g(x) fracdxZ - fracG^2Z^2","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"By substituting rho_f(x)=f(x) and  rho_g(x)=g(x), the variances of F_rm MC and G_rm MC are given by","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"sigma^2_F_rm MC= frac1Z^2 left( Z Z_f - F^2 right)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"sigma^2_G_rm MC= frac1Z^2 left( Z Z_g - G^2 right)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"We derive the variance of F as","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracsigma^2_FF^2 = fracZ cdot Z_fF^2+fracZ cdot Z_gG^2 - 2 ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Note that g(x)0 indicates Z_g = G,  so that","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracsigma^2_FF^2 = fracZ_f^2F^2+fracGcdot Z_fF^2+fracZ_fG - 1","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Interestingly, this variance is a function of G instead of a functional of g(x). It is then possible to normalized g(x) with a constant to minimize the variance. The optimal constant makes G to be,","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracd sigma^2_FdG=0","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"which makes G_best = F. The minimized the variance is given by,","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracsigma^2_FF^2= left(fracZ_fF+1right)^2 - 2ge 0","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"The equal sign is achieved when f(x)0 is positively defined.","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"It is very important that the above analysis is based on the assumption that the autocorrelation time negligible. The autocorrelation time related to the jump between the normalization and physical sectors is controlled by the deviation of the ratio f(x)g(x) from unity. The variance sigma_F^2 given above will be amplified to sim sigma_F^2 tau where tau is the autocorrelation time.","category":"page"},{"location":"man/important_sampling/#Approach-2:-Conventional-algorithm-(e.g.,-Vegas-algorithm)","page":"Important Sampling","title":"Approach 2: Conventional algorithm (e.g., Vegas algorithm)","text":"","category":"section"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Important sampling is actually more straightforward than the above approach. One simply sample x with a distribution rho_g(x)=g(x)Z_g, then measure the observable f(x)g(x). Therefore, the mean estimation,","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracFZ=int dx fracf(x)g(x) rho_g(x)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"the variance of F in this approach is given by,","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"sigma_F^2=Z_g^2int dx left( fracf(x)g(x)- fracFZ_gright)^2rho_g(x)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracsigma_F^2F^2=fracZ_gF^2int dx fracf(x)^2g(x)- 1","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"The optimal g(x) that minimizes the variance is g(x) =f(x),","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"fracsigma_F^2F^2=fracZ_f^2F^2-1","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"The variance of the conventional approach is a functional of g(x), while that of the previous approach isn't. There are two interesting limit:\nIf the f(x)0, the optimal choice g(x)=f(x) leads to zero variance. In this limit, the conventional approach is clearly much better than the previous approach.\nOn the other hand, if g(x) is far from the optimal choice f(x), say simply setting g(x)=1, one naively expect that the the conventional approach may leads to much larger variance than the previous approach. However,  this statement may not be true. If g(x) is very different from f(x), the normalization and the physical sector in the previous approach mismatch, causing large autocorrelation time and large statistical error . In contrast, the conventional approach doesn't have this problem.","category":"page"},{"location":"man/important_sampling/#Benchmark","page":"Important Sampling","title":"Benchmark","text":"","category":"section"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"To benchmark, we sample the following integral up to 10^8 updates, ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"int_0^beta e^-(x-beta2)^2delta^2dx approx sqrtpidelta","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"where beta gg delta.","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"g(x)=f(x)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Normalization Sector:  doesn't lead to exact result, the variance left(fracZ_fF+1right)^2 - 2=2 doesn't change with parameters","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"beta 10 100\nresult 0.1771(1) 0.1773(1)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Conventional: exact result","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"g(x)=sqrtpideltabeta1","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"beta 10 100\nNormalization 0.1772(4) 0.1767(17)\nConventional 0.1777(3) 0.1767(8)","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"g(x)=exp(-(x-beta2+s)^2delta^2) with beta=100","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"s delta 2delta 3delta 4delta 5delta\nNormalization 0.1775(8) 0.1767(25) 0.1770(60) 0.176(15) 183(143)\nConventional 0.1776(5) 0.1707(39) 0.1243(174) 0.0204 (64) ","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"The conventional algorithm is not ergodic anymore for s=4delta, the acceptance ratio to update x is about 015, while the normalization algorithm becomes non ergodic for s=5delta. So the latter is slightly more stable.","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"<!– The code are ![[test.jl]] for the normalization approach and ![[test2.jl]] for the conventional approach. –>","category":"page"},{"location":"man/important_sampling/","page":"Important Sampling","title":"Important Sampling","text":"Reference:  [1] Wang, B.Z., Hou, P.C., Deng, Y., Haule, K. and Chen, K., Fermionic sign structure of high-order Feynman diagrams in a many-fermion system. Physical Review B, 103, 115141 (2021).","category":"page"},{"location":"lib/mcmc/#Markov-chain-Monte-Carlo","page":"Markov-chain Monte Carlo","title":"Markov-chain Monte Carlo","text":"","category":"section"},{"location":"lib/mcmc/","page":"Markov-chain Monte Carlo","title":"Markov-chain Monte Carlo","text":"Modules = [MCIntegration.MCMC]","category":"page"},{"location":"lib/mcmc/#MCIntegration.MCMC.montecarlo-Union{Tuple{T}, Tuple{O}, Tuple{P}, Tuple{V}, Tuple{N}, Tuple{Configuration{N, V, P, O, T}, Function, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any, Any, Any}} where {N, V, P, O, T}","page":"Markov-chain Monte Carlo","title":"MCIntegration.MCMC.montecarlo","text":"function montecarlo(config::Configuration{N,V,P,O,T}, integrand::Function, neval,\n    print=0, save=0, timer=[], debug=false;\n    measurefreq=2, measure::Union{Nothing,Function}=nothing, kwargs...) where {N,V,P,O,T}\n\nThis algorithm calculate high-dimensional integrals with a Markov-chain Monte Carlo. For multiple integrands invoves multiple variables, it finds the best distribution ansatz to fit them all together. In additional to the original integral, it also  introduces a normalization integral with integrand ~ 1.\n\nAssume we want to calculate the integral f_1(x) and f_2(x y), where x, y are two different types of variables. The algorithm will try to learn a distribution rho_x(x) and rho_y(y) so that f_1(x)rho_x(x) and f_2(x y)rho_x(x)rho_y(y) are as flat as possible. \n\nThe algorithm then samples the variables x and y with a joint distribution using the Metropolis-Hastings algorithm, math p(x y) = r_0 cdot rho_x(x) cdot rho_y(y) + r_1 cdot f_1(x) cdot rho_y(y) + r_2 cdot f_2(x y) where r_i are certain reweighting factor to make sure all terms contribute same weights. One then estimate the integrals by averaging the observables f_1(x)rho_y(y)p(x y) and f_2(x y)p(x y).\n\nThis algorithm reduces to the vanilla Vegas algorithm by setting r_0 = 1 and r_i0 = 0.\n\nThe key difference between this algorithmm and the algorithm :vegasmc is the way that the joint distribution p(x y) is sampled. In this algorithm, one use Metropolis-Hastings algorithm to sample each term in p(x y) as well as the variables (x y), so that the MC configuration space consists of (idx x y), where idx is the index of the user-defined and the normalization integrals. On the other hand, the :vegasmc algorithm only uses Metropolis-Hastings algorithm to sample the configuration space (x y). For a given set of x and y, all terms in p(x y) are explicitly calculated on the fly. If one can afford calculating all the integrands on the fly, then :vegasmc should be more efficient than this algorithm.\n\nNOTE: If there are more than one integrals, only one of them are sampled and measured at each Markov-chain Monte Carlo step!\n\nFor low-dimensional integrations, this algorithm is much less efficient than the :vegasmc or :vegas solvers. For high-dimension integrations, however, this algorithm becomes as efficent and robust as the :vegasmc solver, and is more efficient and robust than the :vegas solver.\n\nArguments\n\nintegrand : User-defined function with the following signature:\n\nfunction integrand(idx, var, config)\n    # calculate your integrand values\n    # return integrand of the index idx\nend\n\nThe first argument idx is index of the integral being sampled. The second parameter var is either a Variable struct if there is only one type of variable, or a tuple of Varibles if there are more than one types of variables. The third parameter passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\n\nmeasure : User-defined function with the following signature:\n\nfunction measure(idx, var, obs, weight, config)\n    # accumulates the weight into the observable\n    # For example,\n    # obs[idx] = weight # integral idx\n    # ...\nend\n\nThe first argument idx is index of the integral being sampled. The second argument var is either a Variable struct if there is only one type of variable, or a tuple of Varibles if there are more than one types of variables. The third argument passes the user-defined observable to the function, it should be a vector with the length same as the integral number. The fourth argument is the integrand weights to be accumulated to the observable, it is a vector with the length same as the integral number. The last argument passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\n\nRemark:\n\nWhat if the integral result makes no sense?\nOne possible reason is the reweight factor. It is important for the Markov chain to visit the integrals with the similar frequency.  However, the weight of different integrals may be order-of-magnitude different. It is thus important to reweight the integrals.  Internally, the MC sampler try to reweight for each iteration. However, it could fail either 1) the total MC steps is too small so that  reweighting doesn't have enough time to show up; ii) the integrals are simply too different, and the internal reweighting subroutine is  not smart enough to figure out such difference. If 1) is the case, one either increase the neval. If 2) is the case, one may mannually  provide an array of reweight factors when initializes the MCIntegration.configuration struct. \n\nExamples\n\nThe following command calls the MC Vegas solver,\n\njulia> integrate((idx, x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = 2, print=-1, solver=:mcmc)\nIntegral 1 = 0.6757665376867902 ± 0.008655534861083898   (chi2/dof = 0.681)\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = MCIntegration","category":"page"},{"location":"#MCIntegration","page":"Home","title":"MCIntegration","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Robust and efficient Monte Carlo calculator for high-dimensional integral.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)","category":"page"},{"location":"","page":"Home","title":"Home","text":"MCIntegration.jl provides several Monte Carlo algorithms to calculate regular/singular integrals in finite or inifinite dimensions.  ","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following examples demonstrate the basic usage of this package. ","category":"page"},{"location":"#One-dimensional-integral","page":"Home","title":"One-dimensional integral","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We first show an example of highly singular integral. The following command evaluates ∫_0^1 log(x)/√x dx = 4.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> res = integrate((x, c)->log(x[1])/sqrt(x[1]), solver=:vegas) \n====================================     Integral 1    ==========================================\n  iter              integral                            wgt average                      chi2/dof\n-------------------------------------------------------------------------------------------------\nignore        -3.8394711 ± 0.12101621              -3.8394711 ± 0.12101621                 0.0000\n     2         -3.889894 ± 0.04161423              -3.8394711 ± 0.12101621                 0.0000\n     3        -4.0258398 ± 0.016628525              -4.007122 ± 0.015441393                9.2027\n     4        -4.0010193 ± 0.0097242712            -4.0027523 ± 0.0082285382               4.6573\n     5         -3.990754 ± 0.0055248673            -3.9944823 ± 0.0045868638               3.5933\n     6         -4.000744 ± 0.0025751679            -3.9992433 ± 0.0022454867               3.0492\n     7        -4.0021542 ± 0.005940518             -3.9996072 ± 0.0021004392               2.4814\n     8        -3.9979708 ± 0.0034603885            -3.9991666 ± 0.0017955468               2.0951\n     9         -3.994137 ± 0.0026675679            -3.9975984 ± 0.0014895459               2.1453\n    10        -3.9999099 ± 0.0033455927            -3.9979808 ± 0.0013607691               1.9269\n-------------------------------------------------------------------------------------------------\nIntegral 1 = -3.997980772652019 ± 0.0013607691354676158   (chi2/dof = 1.93)","category":"page"},{"location":"","page":"Home","title":"Home","text":"By default, the function performs 10 iterations and each iteraction costs about 1e4 evaluations. You may reset these values with niter and neval keywords arguments.\nThe final result is obtained by inverse-variance-weighted averge of all the iterations except the first one (there is no important sampling yet!). They are stored in the return valueres, which is a struct Result. You can access the statistics with res.mean, res.stdev, res.chi2, res.dof and res.iterations for all the iterations. You can print them on the screen using report(res).\nIf you want to exclude more iterations, say the first three iterations, you can get a new result with the call Result(res, 3).  \nInternally, the integrate function optimizes the important sampling after each iteration. The results generally improves with iteractions. As long as neval is sufficiently large, the estimations from different iteractions should be statistically independent. This will justify an average of different iterations weighted by the inverse variance. The assumption of statically independence can be explicitly checked with chi-square test, namely chi2/dof should be about one. \nYou can pass the keyword arguemnt solver to the integrate functoin to specify the Monte Carlo algorithm. The above examples uses the Vegas algorithm with :vegas. In addition, this package provides two Markov-chain Monte Carlo algorithms for numerical integration. You can call them with :vegasmc or :mcmc. Check the Algorithm section for more details. \nFor the :vegas and vegasmc algorithms, the user-defined integrand evaluation function requires two arguments (x, c), where x is the integration variable, while c is a struct stores the MC configuration. The latter contains additional information which may be needed for integrand evalution.  \nThe 'Configuration' struct stores the essential state information for the Monte Carlo sampling. Two particularly relavent members are\nuserdata : if you pass a keyword argument userdata to the integrate function, then it will be stored here, so that you can access it in your integrand evaluation function. \nvar : A tuple of variables. In the above example, var = (x, ) so that var[1] === x. \nThe result returned by the integrate function contains the configuration after integration. If you want a detailed report, call report(res.config). This configuration stores the optimized random variable distributions for the important sampling, which could be useful to evaluate other integrals with similar integrands. To use the optimized distributions, you can either call integrate(..., config = res.config, ...) to pass the entire configuration, or call integrate(..., var = (res.config.var[1], ...), ...) to pass one or more selected variables.","category":"page"},{"location":"#Multi-dimensional-integral","page":"Home","title":"Multi-dimensional integral","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following example first defines a pool of variables in [0, 1), then evaluates the area of a quarter unit circle (π/4 = 0.785398...).","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> X=Continuous(0.0, 1.0) #Create a pool of continuous variables. It supports as much as 16 same type of variables. see the section [variable](#variable) for more details.\nAdaptive continuous variable in the domain [0.0, 1.0). Max variable number = 16. Learning rate = 2.0.\n\njulia> integrate((X, c)->(X[1]^2+X[2]^2<1.0); var = X, dof = 2, print=-1) # print=-1 minimizes the output information\nIntegral 1 = 0.7832652785953883 ± 0.002149843816733503   (chi2/dof = 1.28)","category":"page"},{"location":"#Multiple-Integrands-Simultaneously","page":"Home","title":"Multiple Integrands Simultaneously","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can calculate multiple integrals simultaneously. If the integrands are similar to each other, evaluating the integrals simultaneously sigificantly reduces cost. The following example calculate the area of a quarter circle and the volume of one-eighth sphere.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> integrate((X, c)->(X[1]^2+X[2]^2<1.0, X[1]^2+X[2]^2+X[3]^2<1.0); var = Continuous(0.0, 1.0), dof = [[2,],[3,]], print=-1)\nIntegral 1 = 0.7823432452235586 ± 0.003174967010742156   (chi2/dof = 2.82)\nIntegral 2 = 0.5185515421806122 ± 0.003219487569949905   (chi2/dof = 1.41)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Here dof defines how many (degrees of freedom) variables of each type. For example, [[n1, n2], [m1, m2], ...] means the first integral involves n1 varibales of type 1, and n2 variables of type2, while the second integral involves m1 variables of type 1 and m2 variables of type 2. The dof of the integrals can be quite different, the program will figure out how to optimally padding the integrands to match the degrees of freedom. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can also use the julia do-syntax to improve the readability of the above example,","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> integrate(var = Continuous(0.0, 1.0), dof = [[2,], [3,]], print = -1) do X, c\n           r1 = (X[1]^2 + X[2]^2 < 1.0) ? 1.0 : 0.0\n           r2 = (X[1]^2 + X[2]^2 + X[3]^2 < 1.0) ? 1.0 : 0.0\n           return (r1, r2)\n       end","category":"page"},{"location":"#Measure-Histogram","page":"Home","title":"Measure Histogram","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You may want to study how an integral changes with a tuning parameter. The following example how to solve histogram measurement problem.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> N = 20;\n\njulia> grid = grid = [i / N for i in 1:N];\n\njulia> function integrand(vars, config)\n            grid = config.userdata # radius\n            x, bin = vars #unpack the variables\n            r = grid[bin[1]] # binned variable in [0, 1)\n            r1 = x[1]^2 + r^2 < 1 # circle\n            r2 = x[1]^2 + x[2]^2 + r^2 < 1 # sphere\n            return r1, r2\n        end;\n\njulia> function measure(vars, obs, weights, config) \n       # obs: prototype of the observables for each integral\n           x, bin = vars #unpack the variables\n           obs[1][bin[1]] += weights[1] # circle\n           obs[2][bin[1]] += weights[2] # sphere\n       end;\n\njulia> res = integrate(integrand;\n                measure = measure, # measurement function\n                var = (Continuous(0.0, 1.0), Discrete(1, N)), # a continuous and a discrete variable pool \n                dof = [[1,1], [2,1]], \n                # integral-1: one continuous and one discrete variables, integral-2: two continous and one discrete variables\n                obs = [zeros(N), zeros(N)], # prototype of the observables for each integral\n                userdata = grid, neval = 1e5, print = -1)\nIntegral 1 = 0.9957805541613277 ± 0.008336657854575344   (chi2/dof = 1.15)\nIntegral 2 = 0.7768105610812656 ± 0.006119386106596811   (chi2/dof = 1.4)","category":"page"},{"location":"","page":"Home","title":"Home","text":"You can visualize the returned result res with Plots.jl. The command res.mean[i] and res.stdev[i] give the mean and stdev of the histogram of the integral i.","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Plots\n\njulia> plt = plot(grid, res.mean[1], yerror = res.stdev[1], xlabel=\"R\", label=\"circle\", aspect_ratio=1.0, xlim=[0.0, 1.0])\n\njulia> plot!(plt, grid, res.mean[2], yerror = res.stdev[2], label=\"sphere\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: histogram)","category":"page"},{"location":"#Algorithm","page":"Home","title":"Algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides three solvers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Vegas algorithm (:vegas): A Monte Carlo algorithm that uses importance sampling as a variance-reduction technique. Vegas iteratively builds up a piecewise constant weight function, represented","category":"page"},{"location":"","page":"Home","title":"Home","text":"on a rectangular grid. Each iteration consists of a sampling step followed by a refinement of the grid. The exact details of the algorithm can be found in G.P. Lepage, J. Comp. Phys. 27 (1978) 192, 3 and G.P. Lepage, Report CLNS-80/447, Cornell Univ., Ithaca, N.Y., 1980. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Vegas algorithm based on Markov-chain Monte Carlo (:vegasmc): A markov-chain Monte Carlo algorithm that uses the Vegas variance-reduction technique. It is as accurate as the vanilla Vegas algorithm, meanwhile tends to be more robust. For complicated high-dimensional integral, the vanilla Vegas algorithm can fail to learn the piecewise constant weight function. This algorithm uses Metropolis–Hastings algorithm to sample the integrand and improves the weight function learning.\nMarkov-chain Monte Carlo (:mcmc): This algorithm is useful for calculating bundled integrands that are too many to calculate at once. Examples are the path-integral of world lines of quantum particles, which involves hundreds and thousands of nested spacetime integrals. This algorithm uses the Metropolis-Hastings algorithm to jump between different integrals so that you only need to evaluate one integrand at each Monte Carlo step. Just as :vegas and :vegasmc, this algorithm also learns a piecewise constant weight function to reduce the variance. However, because it assumes you can access one integrand at each step, it tends to be less accurate than the other two algorithms for low-dimensional integrals.   ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The signature of the integrand and measure functions of the :mcmc solver receices an additional index argument than that of the :vegas and :vegasmc solvers. As shown in the above examples, the integrand and measure functions of the latter two solvers should be like integrand( vars, config) and measure(vars, obs, weights, config), where weights is a vectors carries the values of the integrands at the current MC step. On the other hand, the :mcmc solver requires something like integrand(idx, vars, config) and measure(idx, vars, weight, config), where idx is the index of the integrand of the current step, and the argument weight is a scalar carries the value of the current integrand being sampled.","category":"page"},{"location":"#Variables","page":"Home","title":"Variables","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package supports a couple of common types random variables. You can create them using the following constructors,","category":"page"},{"location":"","page":"Home","title":"Home","text":"Continous(lower, upper[; adapt = true, alpha = 3.0, ...]): Continuous real-valued variables on the domain [lower, upper). MC will learn the distribution using the Vegas algorithm and then perform an imporant sampling accordingly.\nDiscrete(lower::Int, upper::Int[; adapt = true, alpha = 3.0, ...]): Integer variables in the closed set [lower, upper]. MC will learn the distribution and perform an imporant sampling accordingly.","category":"page"},{"location":"","page":"Home","title":"Home","text":"After each iteration, the code will try to optimize how the variables are sampled, so that the most important regimes of the integrals will be sampled most frequently. Setting alpha to be true/false will turn on/off this distribution learning. The parameter alpha controls the learning rate.","category":"page"},{"location":"","page":"Home","title":"Home","text":"When you call the above constructor, it creates a unlimited pool of random variables of a given type. The size of the pool will be dynamically determined when you call a solver. All variables in this pool will be sampled with the same distribution. In many high-dimensional integrals, many variables of integration may contribute to the integral in a similar way, then they can be sampled from the same variable pool. For example, in the above code example, the integral for the circle area and the sphere volume both involve the variable type Continuous. The former has dof=2, while the latter has dof=3. To evaluate a given integrand, you only need to choose some of variables to evaluate a given integral. The rest of the variables in the pool serve as dummy ones. They will not cause any computational overhead.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The variable pool trick will siginicantly reduce the cost of learning their distribution. It also opens the possibility to calculate integrals with infinite dimensions (for example, the path-integral of particle worldlines in quantum many-body physics). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"If some of the variables are paired with each other (for example, the three continuous variables (r, θ, ϕ) representing a 3D vector), then you can pack them into a joint random variable, which can be constructed with the following constructor,","category":"page"},{"location":"","page":"Home","title":"Home","text":"CompositeVar(var1, var2, ...[; adapt = true, alpha = 3.0, ...]): A produce of different types of random variables. It samples var1, var2, ... with their producted distribution. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The packed variables will be sampled all together in the Markov-chain based solvers (:vegasmc and :mcmc). Such updates will generate more independent samples compared to the unpacked version. Sometimes, it could reduce the auto-correlation time of the Markov chain and make the algorithm more efficient.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Moreover, packed variables usually indicate nontrivial correlations between their distributions. In the future, it will be interesting to learn such correlation so that one can sample the packed variables more efficiently.","category":"page"},{"location":"","page":"Home","title":"Home","text":"tip: Integrate over different domains\nIf you want to compute an integral over a semi-infinite or an inifite domain, you can follow this advice from Steven G. Johnson: to compute an integral over a semi-infinite interval, you can perform the change of variables x=a+y(1-y):int_a^infty f(x)mathrmdx = int_0^1\nfleft(a + fracy1 - yright)frac1(1 - y)^2mathrmdyFor an infinite interval, you can perform the change of variables x=(2y - 1)((1 - y)y):int_-infty^infty f(x)mathrmdx = int_0^1\nfleft(frac2y - 1(1 - y)yright)frac2y^2 - 2y + 1(1 -\ny)^2y^2mathrmdyIn addition, recall that for an even function int_-infty^infty f(x)mathrmdx = 2int_0^inftyf(x)mathrmdx,  while the integral of an odd function over the infinite interval (-infty infty) is zero.","category":"page"},{"location":"#Parallelization","page":"Home","title":"Parallelization","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MCIntegration supports MPI parallelization. To run your code in MPI mode, simply use the command","category":"page"},{"location":"","page":"Home","title":"Home","text":"mpiexec julia -n #NCPU ./your_script.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"where #CPU is the number of workers. Internally, the MC sampler will send the blocks (controlled by the argument Nblock, see above example code) to different workers, then collect the estimates in the root node. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that you need to install the package MPI.jl to use the MPI mode. See this link for the instruction on the configuration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The user essentially doesn't need to write additional code to support the parallelization. The only tricky part is the output: only the function MCIntegratoin.integrate of the root node returns meaningful estimates, while other workers simply returns nothing. ","category":"page"}]
}
