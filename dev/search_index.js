var documenterSearchIndex = {"docs":
[{"location":"lib/vegas/#Vegas-algorithm","page":"Vegas algorithm","title":"Vegas algorithm","text":"","category":"section"},{"location":"lib/vegas/#MCIntegration.Vegas.montecarlo-Union{Tuple{T}, Tuple{O}, Tuple{P}, Tuple{V}, Tuple{Ni}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Any, Any, Any}, Tuple{Configuration{Ni, V, P, O, T}, Function, Vararg{Any, 4}}} where {Ni, V, P, O, T}","page":"Vegas algorithm","title":"MCIntegration.Vegas.montecarlo","text":"function montecarlo(config::Configuration{Ni,V,P,O,T}, integrand::Function, neval,\n    verbose=0, timer=[], debug=false;\n    measure::Union{Nothing,Function}=nothing, measurefreq::Int=1) where {Ni,V,P,O,T}\n\nThis function implements the Vegas algorithm, a Monte Carlo method specifically designed for multi-dimensional integration. The underlying principle and methodology of the algorithm can be explored further in the Vegas documentation.\n\nOverview\n\nThe Vegas algorithm employs an importance sampling scheme. For a one-dimensional integral with the integrand f(x), the algorithm constructs an optimized distribution rho(x) that approximates the integrand as closely as possible (a strategy known as the Vegas map trick; refer to Dist.Continuous for more details).\n\nThe variable x is then sampled using the distribution rho(x), and the integral is estimated by averaging the estimator f(x)rho(x).\n\nNote\n\nIf there are multiple integrals, all of them are sampled and measured at each Monte Carlo step.\nThis algorithm is particularly efficient for low-dimensional integrations but might be less efficient and robust than the Markov-chain Monte Carlo solvers for high-dimensional integrations.\n\nArguments\n\nintegrand: A user-defined function evaluating the integrand. The function should be either integrand(var, config) or integrand(var, weights, config) depending on whether inplace is false or true respectively. Here, var are the random variables and weights is an output array to store the calculated weights. The last parameter passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\nmeasure: An optional user-defined function to accumulate the integrand weights into the observable. The function signature should be measure(var, obs, relative_weights, config). Here, obs is a vector of observable values for each component of the integrand and relative_weights are the weights calculated from the integrand multiplied by the probability of the corresponding variables.\n\nThe following are the snippets of the integrand and measure functions:\n\nfunction integrand(var, config)\n    # calculate your integrand values\n    # return integrand1, integrand2, ...\nend\n\nfunction measure(var, obs, weights, config)\n    # accumulates the weight into the observable\n    # For example,\n    # obs[1] = weights[1] # integral 1\n    # obs[2] = weights[2] # integral 2\n    # ...\nend\n\nExamples\n\nThe following command calls the Vegas solver,\n\njulia> integrate((x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = [[2,],], verbose=-1, solver=:vegas)\nIntegral 1 = 0.667203631824444 ± 0.0005046485925614018   (reduced chi2 = 1.46)\n\n\n\n\n\n","category":"method"},{"location":"lib/utility/#Utility","page":"Utility","title":"Utility","text":"","category":"section"},{"location":"lib/utility/#MCIntegration.MCUtility","page":"Utility","title":"MCIntegration.MCUtility","text":"Utility data structures and functions\n\n\n\n\n\n","category":"module"},{"location":"lib/utility/#MCIntegration.MCUtility.StopWatch","page":"Utility","title":"MCIntegration.MCUtility.StopWatch","text":"StopWatch(start, interval, callback)\n\nInitialize a stopwatch. \n\nArguments\n\nstart::Float64: initial time (in seconds)\ninterval::Float64 : interval to click (in seconds)\ncallback : callback function after each click (interval seconds)\n\n\n\n\n\n","category":"type"},{"location":"lib/utility/#MCIntegration.MCUtility.MPIbcast!-Tuple{AbstractArray}","page":"Utility","title":"MCIntegration.MCUtility.MPIbcast!","text":"function MPIbcast!(data::AbstractArray)\n\nBroadcast data from MPI root to other nodes. data is an array.\n\n\n\n\n\n","category":"method"},{"location":"lib/utility/#MCIntegration.MCUtility.MPIbcast-Tuple{Any}","page":"Utility","title":"MCIntegration.MCUtility.MPIbcast","text":"function MPIbcast(data)\n\nBroadcast data from MPI root to other nodes. data can be an array or a scalar. The root node its own data, and other nodes return the broadcasted data from the root.\n\n\n\n\n\n","category":"method"},{"location":"lib/utility/#MCIntegration.MCUtility.MPIreduce","page":"Utility","title":"MCIntegration.MCUtility.MPIreduce","text":"function MPIreduce(data, op = MPI.SUM)\n\nReduce data from MPI workers to root with the operation op. data can be an array or a scalar. The root node returns the reduced data with the operation op, and other nodes return their own data.\n\n\n\n\n\n","category":"function"},{"location":"lib/utility/#MCIntegration.MCUtility.MPIreduce!","page":"Utility","title":"MCIntegration.MCUtility.MPIreduce!","text":"function MPIreduce!(data::AbstractArray, op = MPI.SUM)\n\nReduce data from MPI workers to root with the operation op. data should be an array.\n\n\n\n\n\n","category":"function"},{"location":"lib/utility/#MCIntegration.MCUtility.check-Tuple{MCIntegration.MCUtility.StopWatch, Vararg{Any}}","page":"Utility","title":"MCIntegration.MCUtility.check","text":"check(stopwatch, parameter...)\n\nCheck stopwatch. If it clicks, call the callback function with the unpacked parameter\n\n\n\n\n\n","category":"method"},{"location":"lib/utility/#MCIntegration.MCUtility.disable_threading-Tuple{}","page":"Utility","title":"MCIntegration.MCUtility.disable_threading","text":"Convenience function to disable all threading and assert that Julia threading is off as well.\n\n\n\n\n\n","category":"method"},{"location":"lib/utility/#MCIntegration.MCUtility.mpi_nprocs","page":"Utility","title":"MCIntegration.MCUtility.mpi_nprocs","text":"Number of processors used in MPI. Can be called without ensuring initialization.\n\n\n\n\n\n","category":"function"},{"location":"lib/vegasmc/#Markov-chain-based-Vegas-algorithm","page":"Markov-chain based Vegas algorithm","title":"Markov-chain based Vegas algorithm","text":"","category":"section"},{"location":"lib/vegasmc/#MCIntegration.VegasMC.montecarlo-Union{Tuple{T}, Tuple{O}, Tuple{P}, Tuple{V}, Tuple{N}, Tuple{Configuration{N, V, P, O, T}, Function, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Vararg{Any, 4}}} where {N, V, P, O, T}","page":"Markov-chain based Vegas algorithm","title":"MCIntegration.VegasMC.montecarlo","text":"function montecarlo(config::Configuration{N,V,P,O,T}, integrand::Function, neval,\n    verbose=0, debug=false;\n    measurefreq::Int=1, measure::Union{Nothing,Function}=nothing) where {N,V,P,O,T}\n\nThis function applies a Markov-chain Monte Carlo (MCMC) technique combined with the Vegas algorithm to compute integrals. In addition to calculating the original integrals, it also introduces a normalization integral with an integrand ~ 1, which enhances the efficiency and robustness of high-dimensional integration tasks.\n\nOverview\n\nGiven multiple integrands involving multiple variables, the algorithm finds the best distribution ansatz that fits all integrands together. For instance, consider we want to calculate two integrals: f_1(x) and f_2(x y), where x and y are two different types of variables. The algorithm learns distributions rho_x(x) and rho_y(y) such that f_1(x)rho_x(x) and f_2(x y)rho_x(x)rho_y(y) are as flat as possible.\n\nThen, it samples variables x and y using the Metropolis-Hastings algorithm with a joint distribution p(x, y),\n\np(x y) = r_0 cdot rho_x(x) cdot rho_y(y) + r_1 cdot f_1(x) cdot rho_y(y) + r_2 cdot f_2(x y)\n\nwhere r_i are certain reweighting factor to make sure all terms contribute same weights.\n\nOne can then estimate the integrals by averaging the observables f_1(x)rho_y(y)p(x y) and f_2(x y)p(x y).\n\nThe algorithm defaults to the standard Vegas algorithm if r_0 = 1 and r_i0 = 0.\n\nArguments\n\nintegrand: A user-defined function evaluating the integrand. The function should be either integrand(var, config) or integrand(var, weights, config) depending on whether inplace is false or true respectively. Here, var are the random variables and weights is an output array to store the calculated weights. The last parameter passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\nmeasure: An optional user-defined function to accumulate the integrand weights into the observable. The function signature should be measure(var, obs, relative_weights, config). Here, obs is a vector of observable values for each component of the integrand and relative_weights are the weights calculated from the integrand multiplied by the probability of the corresponding variables.\n\nThe following are the snippets of the integrand and measure functions:\n\nfunction integrand(var, config)\n    # calculate your integrand values\n    # return integrand1, integrand2, ...\nend\n\nfunction measure(var, obs, weights, config)\n    # accumulates the weight into the observable\n    # For example,\n    # obs[1] = weights[1] # integral 1\n    # obs[2] = weights[2] # integral 2\n    # ...\nend\n\nExamples\n\nThe following command calls the VegasMC solver,\n\njulia> integrate((x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = [[2,],], verbose=-1, solver=:vegasmc)\nIntegral 1 = 0.6640840471808533 ± 0.000916060916265263   (reduced chi2 = 0.945)\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#Random-Variables","page":"Random Variables","title":"Random Variables","text":"","category":"section"},{"location":"lib/distribution/#MCIntegration.Dist.CompositeVar","page":"Random Variables","title":"MCIntegration.Dist.CompositeVar","text":"mutable struct CompositeVar{V}\n\nA composite variable is a tuple of variables. The probability of the composite variable is the product of the probabilities of the bundled variables.\n\nFields:\n\nvars   : tuple of Variables\nprob   : probability of the composite variable\noffset : offset of the variable pool, all variables in the pool share the same offset\nadapt  : turn the adaptive map on or off\nsize   : size of each variable pool, all variables in the pool share the same size\n_prob_cache : cache of the probability of the composite variable\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.CompositeVar-Tuple","page":"Random Variables","title":"MCIntegration.Dist.CompositeVar","text":"function CompositeVar(vargs...; adapt=true)\n\nCreate a product of different types of random variables. The bundled variables will be sampled with the product of their distributions.\n\nArguments:\n\nvargs  : tuple of Variables\nadapt  : turn the adaptive map on or off\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.Continuous","page":"Random Variables","title":"MCIntegration.Dist.Continuous","text":"function Continuous(lower, upper, size=MaxOrder; offset=0, alpha=2.0, adapt=true, ninc=1000, grid=collect(LinRange(lower, upper, ninc)))\n\nCreate a pool of continuous variables sampled from the set [lower, upper) with a distribution generated by a Vegas map (see below).  The distribution is trained after each iteration if adapt = true.\n\nArguments:\n\nlower  : lower bound\nupper  : upper bound\nninc   : number of increments\nalpha  : learning rate\nadapt  : turn the adaptive map on or off\ngrid   : grid points for the vegas map\n\nRemark:\n\nVegas map maps the original integration variables x into new variables y, so that the integrand is as flat as possible in y:\n\nbeginaligned\nx_0 = a \nx_1 = x_0 + Delta x_0 \nx_2 = x_1 + Delta x_1 \ncdots \nx_N = x_N-1 + Delta x_N-1 = b\nendaligned\n\nwhere a and b are the limits of integration. The grid specifies the transformation function at the points y=iN for i=01ldots N:\n\nx(y=iN) = x_i\n\nLinear interpolation is used between those points. The Jacobian for this transformation is:\n\nJ(y) = J_i = N Delta x_i\n\nThe grid point x_i is trained after each iteration.\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.Continuous-2","page":"Random Variables","title":"MCIntegration.Dist.Continuous","text":"function Continuous(bounds::AbstractVector{Union{AbstractVector,Tuple}}, size=MaxOrder; offset=0, alpha=2.0, adapt=true, ninc=1000, grid=collect(LinRange(lower, upper, ninc)))\n\nCreate a set of continuous variable pools sampling from the set [lower, upper) with a distribution generated by a Vegas map, and pack it into a CompositeVar. The distribution is trained after each iteration if adapt = true. \n\nArguments:\n\nbounds : tuple of (lower, upper) for each continuous variable\nninc   : number of increments\nalpha  : learning rate\nadapt  : turn the adaptive map on or off\ngrid   : grid points for the vegas map\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.Continuous-3","page":"Random Variables","title":"MCIntegration.Dist.Continuous","text":"mutable struct Continuous{G} <: AbstractVectorVariable{Float64}\n\nA continuous variable pool is a set of floating point variables sampled from the set [lower, upper) with a distribution generated by a Vegas map (see below). The distribution is trained after each iteration if adapt = true.\n\nFields:\n\ndata   : floating point variables\ngidx   : index of the grid point for each variable\nprob   : probability of the given variable. For the vegas map, = dy/dx = 1/N/Δxᵢ = inverse of the Jacobian\nlower  : lower bound\nrange  : upper - lower\noffset : offset of the variable pool, all variables in the pool share the same offset\ngrid   : grid points for the vegas map\ninc    : increment of the grid points\nhistogram : histogram of the distribution\nalpha  : learning rate\nadapt  : turn the adaptive map on or off\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.Discrete","page":"Random Variables","title":"MCIntegration.Dist.Discrete","text":"function Continuous(bounds::AbstractVector{Union{AbstractVector,Tuple}}, size=MaxOrder; offset=0, alpha=2.0, adapt=true, ninc=1000, grid=collect(LinRange(lower, upper, ninc)))\n\nCreate a set of continuous variable pools sampling from the set [lower, upper) with a distribution generated by a Vegas map, and pack it into a CompositeVar. The distribution is trained after each iteration if adapt = true. \n\nArguments:\n\nbounds : tuple of (lower, upper) for each continuous variable\nninc   : number of increments\nalpha  : learning rate\nadapt  : turn the adaptive map on or off\ngrid   : grid points for the vegas map\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.Discrete-2","page":"Random Variables","title":"MCIntegration.Dist.Discrete","text":"mutable struct Discrete <: AbstractVectorVariable{Int}\n\nA discrete variable pool is a set of integer variables sampled from the closed set [lower, lower+1, ..., upper] with a distribution generated by an adaptive distribution. The distribution is trained after each iteration if adapt = true.\n\nFields:\n\ndata   : integer variables\nlower  : lower bound\nupper  : upper bound\nprob   : probability of the given variable\nsize   : upper-lower+1\noffset : offset of the variable pool, all variables in the pool share the same offset\nhistogram : histogram of the distribution\naccumulation : accumulation of the distribution\ndistribution : distribution of the variable pool\nalpha  : learning rate\nadapt  : turn the adaptive map on or off\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.Discrete-3","page":"Random Variables","title":"MCIntegration.Dist.Discrete","text":"function Discrete(lower::Int, upper::Int; distribution=nothing, alpha=2.0, adapt=true)\n\nCreate a pool of integer variables sampled from the closed set [lower, lower+1, ..., upper] with the distribution Discrete.distribution.  The distribution is trained after each iteration if adapt = true.\n\nArguments:\n\nlower  : lower bound\nupper  : upper bound\ndistributin   : inital distribution \nalpha  : learning rate\nadapt  : turn the adaptive map on or off\n\n\n\n\n\n","category":"type"},{"location":"lib/distribution/#MCIntegration.Dist.accumulate!-Tuple{Any, Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.accumulate!","text":"accumulate!(var, idx, weight) = nothing\n\nAccumulate a new sample with the a given weight for the idx-th element of the Variable pool var.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.clearStatistics!-Tuple{Any}","page":"Random Variables","title":"MCIntegration.Dist.clearStatistics!","text":"clearStatistics!(T)\n\nClear the accumulated samples in the Variable.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.initialize!-Tuple{Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.initialize!","text":"initialize!(T, config)\n\nInitialize the variable pool with random variables.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.locate-Tuple{AbstractVector, Number}","page":"Random Variables","title":"MCIntegration.Dist.locate","text":"function locate(accumulation, p)\n\nReturn index of p in accumulation so that accumulation[idx]<=p<accumulation[idx+1].  If p is not in accumulation (namely accumulation[1] > p or accumulation[end] <= p), return -1. Bisection algorithmn is used so that the time complexity is O(log(n)) with n=length(accumulation).\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.padding_probability-Tuple{Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.padding_probability","text":"padding_probability(config, idx)\n\nCalculate the joint probability of missing variables for the idx-th integral compared to the full variable set.\n\npadding_probability(config, idx) = total_probability(config) / probability(config, idx)\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.poolsize-Tuple{CompositeVar}","page":"Random Variables","title":"MCIntegration.Dist.poolsize","text":"poolsize(vars::CompositeVar) = vars.size\n\nReturn the size of the variable pool. All variables packed in the CompositeVar share the same size.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.poolsize-Union{Tuple{MCIntegration.Dist.AbstractVectorVariable{GT}}, Tuple{GT}} where GT","page":"Random Variables","title":"MCIntegration.Dist.poolsize","text":"function poolsize(var::AbstractVectorVariable{GT}) where {GT}\n\nReturn the size of the pool of the variable.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.probability-Tuple{Any, Any}","page":"Random Variables","title":"MCIntegration.Dist.probability","text":"probability(config, idx)\n\nCalculate the joint probability of all involved variable for the idx-th integral.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.rescale","page":"Random Variables","title":"MCIntegration.Dist.rescale","text":"function rescale(dist::AbstractVector, alpha=1.5)\n\nRescale the dist array to avoid overreacting to atypically large number.\n\nThere are three steps:\n\ndist will be first normalize to [0, 1].\nThen the values that are close to 1.0 will not be changed much, while that close to zero will be amplified to a value controlled by alpha.\nIn the end, the rescaled dist array will be normalized to [0, 1].\n\nCheck Eq. (19) of https://arxiv.org/pdf/2009.05112.pdf for more detail\n\n\n\n\n\n","category":"function"},{"location":"lib/distribution/#MCIntegration.Dist.smooth","page":"Random Variables","title":"MCIntegration.Dist.smooth","text":"function smooth(dist::AbstractVector, factor=6)\n\nSmooth the distribution by averaging two nearest neighbor. The average ratio is given by 1 : factor : 1 for the elements which are not on the boundary.\n\n\n\n\n\n","category":"function"},{"location":"lib/distribution/#MCIntegration.Dist.total_probability-Tuple{Any}","page":"Random Variables","title":"MCIntegration.Dist.total_probability","text":"total_probability(config)\n\nCalculate the joint probability of all involved variables of all integrals.\n\n\n\n\n\n","category":"method"},{"location":"lib/distribution/#MCIntegration.Dist.train!-Tuple{Any}","page":"Random Variables","title":"MCIntegration.Dist.train!","text":"train!(Var)\n\nTrain the distribution of the variables in the pool.\n\n\n\n\n\n","category":"method"},{"location":"lib/montecarlo/#Main-module","page":"Main module","title":"Main module","text":"","category":"section"},{"location":"lib/montecarlo/#MCIntegration.Configuration","page":"Main module","title":"MCIntegration.Configuration","text":"mutable struct Configuration{NI,V,P,O,T}\n\nStruct that holds all the necessary parameters and variables for Monte Carlo integration.\n\nParameters\n\nNI : Number of integrands\nV : Type of variables\nP : Type of user-defined data\nO : Type of observables \nT : Type of integrand\n\nStatic parameters\n\nseed: Seed to initialize the random number generator, also serves as the unique process ID of the configuration.\nrng: A MersenneTwister random number generator, seeded by seed.\nuserdata: User-defined parameter.\nvar: Tuple of variables. Each variable should derive from the abstract type Variable (see variable.jl for details). Using a tuple instead of a vector improves performance.\n\nIntegrands properties\n\nneighbor::Vector{Vector{Int}} : A vector of integer pairs defining the adjacency of integrands within the Markov chain's undirected transition graph. To ensure the ergodicity of the simulation, the graph must be fully connected, and it is mandatory to include the (N+1)-th diagram as the normalization sector.   Only correlated integrands should be linked as neighbors to optimize transition acceptance rates. By default, the system assumes a linear chain topology [[N+1, 1], [1, 2], ..., [N-1, N]] that sequentially connects all integrands (1 to N) and the normalization integrand (N+1).\ndof::Vector{Vector{Int}}: Degrees of freedom of each integrand, i.e., the dimensions in which each integrand can vary.\nobservable: Observables required to calculate the integrands, will be used in the measure function call.\nreweight: Reweight factors for each integrand. The reweight factor of the normalization integrand (namely, the last element) is assumed to be 1.\nvisited: The number of times each integrand is visited by the Markov chain.\n\nCurrent MC state\n\nstep: The number of Monte Carlo updates performed thus far.\nnorm: The index of the normalization integrand. norm is larger than the index of any user-defined integrands.\nnormalization: The accumulated normalization factor.\npropose/accept: Arrays to store the proposed and accepted updates for each integrand and variable.\n\n\n\n\n\n","category":"type"},{"location":"lib/montecarlo/#MCIntegration.Configuration-Union{Tuple{}, Tuple{V}} where V","page":"Main module","title":"MCIntegration.Configuration","text":"function Configuration(;\n    var::Union{Variable,AbstractVector,Tuple}=(Continuous(0.0, 1.0),),\n    dof::Union{Int,AbstractVector,AbstractMatrix}=[ones(Int, length(var))],\n    type=Float64,  # type of the integrand\n    obs::AbstractVector=zeros(Float64, length(dof)),\n    reweight::Vector{Float64}=ones(length(dof) + 1),\n    seed::Int=rand(Random.RandomDevice(), 1:1000000),\n    userdata=nothing,\n    neighbor::Union{Vector{Vector{Int}},Vector{Tuple{Int,Int}},Nothing}=nothing,\n    kwargs...\n)\n\nCreate a Configuration struct for MC integration.\n\nArguments\n\nvar: Either a single Variable, a CompositeVar, or a tuple consisting of Variable and/or CompositeVar instances. Tuples are used to improve performance by ensuring type stability. By default, var is set to a tuple containing a single continuous variable, (Continuous(0.0, 1.0),).\ndof: Degrees of freedom for each integrand, as a vector of integers. For example, [[0, 1], [2, 3]] means the first integrand has zero instances of var#1 and one of var#2; while the second integrand has two instances of var#1 and 3 of var#2. Defaults to [ones(length(var))], i.e., one degree of freedom for each variable.\ntype: Type of the integrand, Float64 by default.\nobs: Vector of observables needed to calculate the integrands, which will be used in the measure function call. \nreweight: Vector of reweight factors for each integrand. By default, all factors are initialized to one. Internally, a reweight factor of 1 will be appended to the end of the reweight vector, which is for the normalization integral.\nseed: Seed for the random number generator. This also serves as the unique identifier of the configuration. If it is nothing, then a random seed between 1 and 1,000,000 is generated. \nuserdata: User data to pass to the integrand and the measurement.\nneighbor: A vector of integer pairs defining the adjacency of integrands within the Markov chain's undirected transition graph. For example, [(1, 2), (2, 3)] means that the first and second integrands, and the second and third integrands, are neighbors. Neighboring integrands are directly connected in the Markov chain. By default, all integrands are connected in ascending order. Note that the normalization integral is automatically appended at the end of the integrand list and is considered as neighbor with the first user-defined integrand.\n\nExample\n\ncfg = Configuration(\n    var = (Continuous(0.0, 1.0), Continuous(-1.0, 1.0)),\n    dof = [[1, 1], [2, 0]],\n    obs = [0.0, 0.0],\n    seed = 1234,\n    neighbor = [(1, 2)]\n)\n\n\n\n\n\n","category":"method"},{"location":"lib/montecarlo/#MCIntegration.Result","page":"Main module","title":"MCIntegration.Result","text":"struct Result{O,C}\n\nthe returned result of the MC integration.\n\nMembers\n\nmean: mean of the MC integration\nstdev: standard deviation of the MC integration samples\nchi2: reduced chi-square of the MC integration samples\nneval: number of evaluations of the integrand\nignore: ignore iterations untill ignore\nconfig: configuration of the MC integration from the last iteration\niterations: list of tuples [(data, error, Configuration), ...] from each iteration\n\n\n\n\n\n","category":"type"},{"location":"lib/montecarlo/#MCIntegration.average","page":"Main module","title":"MCIntegration.average","text":"function average(history, idx=1; init=1, max=length(history))\n\nAverage the history[1:max]. Return the mean, standard deviation and chi2 of the history.\n\nArguments\n\nhistory: a list of tuples, such as [(data, error, Configuration), ...]\nidx: the index of the integral\nmax: the last index of the history to average with\ninit : the first index of the history to average with\n\n\n\n\n\n","category":"function"},{"location":"lib/montecarlo/#MCIntegration.integrate-Tuple{Function}","page":"Main module","title":"MCIntegration.integrate","text":"function integrate(integrand::Function;\n    solver::Symbol=:vegas, # :mcmc, :vegas, or :vegasmc\n    config::Union{Configuration,Nothing}=nothing,\n    neval=1e4, \n    niter=10, \n    block=16, \n    measure::Union{Nothing,Function}=nothing,\n    measurefreq::Int=1,\n    thermal_ratio::Float64=0.1,\n    inplace::Bool=false,\n    adapt=true,\n    gamma=1.0, \n    reweight_goal::Union{Vector{Float64},Nothing}=nothing, \n    parallel::Symbol=:nothread,\n    ignore::Int=adapt ? 1 : 0,\n    debug=false, \n    verbose=-1, \n    kwargs...\n)\n\nCalculate the integrals, collect statistics, and return a Result struct containing the estimates and errors.\n\nArguments\n\nintegrand: A user-provided function to compute the integrand values. The function signature differs based on the selected solver and whether computations are done in-place:\nFor solver = :vegas or :vegasmc, the function should be either integrand(var, config) or integrand(var, weights, config) depending on whether inplace is false or true respectively. Here, var are the random variables and weights is an output array to store the calculated weights.\nFor solver = :mcmc, the function should be integrand(idx, var, config), where idx is the index of the integrand component to be evaluated.\n\nKeyword Arguments\n\nsolver: Integration algorithm to use: :vegas, :vegasmc, or :mcmc. Default is :vegas.\nconfig: Configuration object for the integration. If nothing, a new one is created using Configuration(; kwargs...).\nneval: Number of integrand evaluations per iteration (default: 1e4).\nniter: Number of iterations for the integration process (default: 10).\nblock: Number of blocks for statistical independence assumption (default: 16).\nmeasure: An optional measurement function. \nFor solver = :vegas or :vegasmc, the function signature should be measure(var, obs, relative_weights, config). Here, obs is a vector of observable values for each component of the integrand and relative_weights are the weights calculated from the integrand multiplied by the probability of the corresponding variables. \nFor solver = :mcmc, the signature should be measure(idx, var, obs, relative_weight, config), where obs is the observable vector and relative_weight is the weight calculated from the idx-th integrand multiplied by the probability of the variables.\nmeasurefreq: How often the measurement function is called (default: 1).\nthermal_ratio : Tha thermalization ratio in one Marovov chain. Default is 0.1.\ninplace: Whether to use the inplace version of the integrand. Default is false, which is more convenient for integrand with a few return values but may cause type instability. Only useful for the :vegas and :vegasmc solver.\nadapt: Whether to adapt the grid and the reweight factor (default: true).\ngamma: Learning rate of the reweight factor after each iteration (default: 1.0).\nreweight_goal: The expected distribution of visited times for each integrand after reweighting. Default is nothing.\nparallel: Run different blocks in parallel. Options are :thread and :nothread. Default is :nothread.\nignore: Ignore the iteration until the ignore round. By default, the first iteration is ignored if adapt=true, and none is ignored if adapt=false.\nverbose: Control the printing level of the iteration history and configuration. \n<-1:print nothing\n-1: print minimal information (Default)\n0: print iteration history\n>0: print MC configuration every verbose seconds and print iteration history. \ndebug: Whether to print debug information such as type instability or float overflow (default: false).\nkwargs: Other keyword arguments for the Configuration constructor.\n\nReturns\n\nReturns a Result struct containing the estimates and errors of the calculated integrals.\n\nNotes\n\nIn MPI mode, only the root process returns meaningful results. All other workers return nothing. Users should handle the returning results properly.\nThe solvers :vegasmc and :vegas automatically append a normalization integral to the end of the integrand vector. When providing reweight_goal, don't forget assign the weight (the last element) for this normalization integral.\n\nExamples\n\nintegrate((x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = [[2,],], verbose=-2, solver=:vegas)\nIntegral 1 = 0.6663652080622751 ± 0.000490978424216832   (reduced chi2 = 0.645)\n\njulia> integrate((x, f, c)-> (f[1] = x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = [[2,],], verbose=-2, solver=:vegas, inplace=true)\nIntegral 1 = 0.6672083165915914 ± 0.0004919147870306026   (reduced chi2 = 2.54)\n\n\n\n\n\n","category":"method"},{"location":"lib/montecarlo/#MCIntegration.report","page":"Main module","title":"MCIntegration.report","text":"function report(result::Result, ignore=result.ignore; pick::Union{Function,AbstractVector}=obs -> first(obs), name=nothing, verbose=0, io::IO=Base.stdout)\n\nprint the summary of the result.  It will first print the configuration from the last iteration, then print the weighted average and standard deviation of the picked observable from each iteration.\n\nArguments\n\nresult: Result object contains the history from each iteration\nignore: ignore the first # iterations.\npick: The pick function is used to select one of the observable to be printed. The return value of pick function must be a Number.\nname: name of each picked observable. If name is not given, the index of the pick function will be used.\n\n\n\n\n\n","category":"function"},{"location":"man/important_sampling/#Important-Sampling","page":"Important Sampling","title":"Important Sampling","text":"","category":"section"},{"location":"man/important_sampling/#Introduction","page":"Important Sampling","title":"Introduction","text":"This note compares two important sampling approaches for Monte Carlo integration. The first approach introduces a normalization sector and lets the Markov chain jumps between this additional sector and the integrand sector following a calibrated probability density for important sampling. One can infer the integration between the ratio of weights between two sectors. On the other hand, the second approach reweights the original integrand to make it as flat as possible, one then perform a random walk uniformly in the parameter space to calculate the integration. This is the conventional approach used in Vegas algorithm.\n\nIn general, the first approach is more robust than the second one, but less efficient. In many applications, for example, high order Feynman diagrams with a sign alternation, the important sampling probability can't represent the complicated integrand well. Then the first approach is as efficient as the second one, but tends to be much robust.\n\nWe next present a benchmark between two approaches. Consider the MC sampling of an one-dimensional functions f(x) (its sign may oscillate).\n\nWe want to design an efficient algorithm to calculate the integral int_a^b dx f(x). To do that, we normalize the integrand with an ansatz g(x)0 to reduce the variant. \n\nOur package supports two important sampling schemes. ","category":"section"},{"location":"man/important_sampling/#Approach-1:-Algorithm-with-a-Normalization-Sector","page":"Important Sampling","title":"Approach 1: Algorithm with a Normalization Sector","text":"In this approach, the configuration spaces consist of two sub-spaces: the physical sector with orders nge 1 and the normalization sector with the order n=0. The weight function of the latter, g(x), should be simple enough so that the integral G=int g(x) d x is explicitly known. In our algorithm we use a constant g(x) propto 1 for simplicity. In this setup, the physical sector weight, namely the integral F = int f(x) dx, can be calculated with the equation\n\n    F=fracF_rm MCG_rm MC G\n\nwhere the MC estimators F_rm MC and G_rm MC are measured with \n\nF_rm MC =frac1N left sum_i=1^N_f fracf(x_i)rho_f(x_i) + sum_i=1^N_g 0 right\n\nand\n\nG_rm MC =frac1N leftsum_i=1^N_f 0 + sum_i=1^N_g fracg(x_i)rho_g(x_i)  right\n\nThe probability density of a given configuration is proportional to rho_f(x)=f(x) and rho_g(x)=g(x), respectively. After N MC updates, the physical sector is sampled for N_f times, and the normalization sector is for N_g times. \n\nNow we estimate the statistic error. According to the propagation of uncertainty, the variance of F  is given by\n\n fracsigma^2_FF^2 =  fracsigma_F_rm MC^2F_MC^2 + fracsigma_G_rm MC^2G_MC^2 \n\nwhere sigma_F_rm MC and sigma_G_rm MC are variance of the MC integration F_rm MC and G_rm MC, respectively. In the Markov chain MC, the variance of F_rm MC can be written as \n\nsigma^2_F_rm MC = frac1N left sum_i^N_f left( fracf(x_i)rho_f(x_i)- fracFZright)^2 +sum_j^N_g left(0-fracFZ right)^2  right \n\n= int left( fracf(x)rho_f(x) - fracFZ right)^2 fracrho_f(x)Z rm dx + int left( fracFZ right)^2 fracrho_g(x)Z dx \n\n=  int fracf^2(x)rho_f(x) fracdxZ -fracF^2Z^2 \n\nHere Z=Z_f+Z_g and Z_fg=int rho_fg(x)dx are the partition sums of the corresponding configuration spaces. Due to the detailed balance, one has Z_fZ_g=N_fN_g.  \n\nSimilarly, the variance of G_rm MC can be written as \n\nsigma^2_G_rm MC=  int fracg^2(x)rho_g(x) fracdxZ - fracG^2Z^2\n\nBy substituting rho_f(x)=f(x) and  rho_g(x)=g(x), the variances of F_rm MC and G_rm MC are given by\n\nsigma^2_F_rm MC= frac1Z^2 left( Z Z_f - F^2 right)\n\nsigma^2_G_rm MC= frac1Z^2 left( Z Z_g - G^2 right)\n\nWe derive the variance of F as\n\nfracsigma^2_FF^2 = fracZ cdot Z_fF^2+fracZ cdot Z_gG^2 - 2 \n\nNote that g(x)0 indicates Z_g = G,  so that\n\nfracsigma^2_FF^2 = fracZ_f^2F^2+fracGcdot Z_fF^2+fracZ_fG - 1\n\nInterestingly, this variance is a function of G instead of a functional of g(x). It is then possible to normalized g(x) with a constant to minimize the variance. The optimal constant makes G to be,\n\nfracd sigma^2_FdG=0\n\nwhich makes G_best = F. The minimized the variance is given by,\n\nfracsigma^2_FF^2= left(fracZ_fF+1right)^2 - 2ge 0\n\nThe equal sign is achieved when f(x)0 is positively defined.\n\nIt is very important that the above analysis is based on the assumption that the autocorrelation time negligible. The autocorrelation time related to the jump between the normalization and physical sectors is controlled by the deviation of the ratio f(x)g(x) from unity. The variance sigma_F^2 given above will be amplified to sim sigma_F^2 tau where tau is the autocorrelation time.","category":"section"},{"location":"man/important_sampling/#Approach-2:-Conventional-algorithm-(e.g.,-Vegas-algorithm)","page":"Important Sampling","title":"Approach 2: Conventional algorithm (e.g., Vegas algorithm)","text":"Important sampling is actually more straightforward than the above approach. One simply sample x with a distribution rho_g(x)=g(x)Z_g, then measure the observable f(x)g(x). Therefore, the mean estimation,\n\nfracFZ=int dx fracf(x)g(x) rho_g(x)\n\nthe variance of F in this approach is given by,\n\nsigma_F^2=Z_g^2int dx left( fracf(x)g(x)- fracFZ_gright)^2rho_g(x)\n\nfracsigma_F^2F^2=fracZ_gF^2int dx fracf(x)^2g(x)- 1\n\nThe optimal g(x) that minimizes the variance is g(x) =f(x),\n\nfracsigma_F^2F^2=fracZ_f^2F^2-1\n\nThe variance of the conventional approach is a functional of g(x), while that of the previous approach isn't. There are two interesting limit:\nIf the f(x)0, the optimal choice g(x)=f(x) leads to zero variance. In this limit, the conventional approach is clearly much better than the previous approach.\nOn the other hand, if g(x) is far from the optimal choice f(x), say simply setting g(x)=1, one naively expect that the the conventional approach may leads to much larger variance than the previous approach. However,  this statement may not be true. If g(x) is very different from f(x), the normalization and the physical sector in the previous approach mismatch, causing large autocorrelation time and large statistical error . In contrast, the conventional approach doesn't have this problem.","category":"section"},{"location":"man/important_sampling/#Benchmark","page":"Important Sampling","title":"Benchmark","text":"To benchmark, we sample the following integral up to 10^8 updates, \n\nint_0^beta e^-(x-beta2)^2delta^2dx approx sqrtpidelta\n\nwhere beta gg delta.\n\ng(x)=f(x)\n\nNormalization Sector:  doesn't lead to exact result, the variance left(fracZ_fF+1right)^2 - 2=2 doesn't change with parameters\n\nbeta 10 100\nresult 0.1771(1) 0.1773(1)\n\nConventional: exact result\n\ng(x)=sqrtpideltabeta1\n\nbeta 10 100\nNormalization 0.1772(4) 0.1767(17)\nConventional 0.1777(3) 0.1767(8)\n\ng(x)=exp(-(x-beta2+s)^2delta^2) with beta=100\n\ns delta 2delta 3delta 4delta 5delta\nNormalization 0.1775(8) 0.1767(25) 0.1770(60) 0.176(15) 183(143)\nConventional 0.1776(5) 0.1707(39) 0.1243(174) 0.0204 (64) \n\nThe conventional algorithm is not ergodic anymore for s=4delta, the acceptance ratio to update x is about 015, while the normalization algorithm becomes non ergodic for s=5delta. So the latter is slightly more stable.\n\n<!– The code are ![[test.jl]] for the normalization approach and ![[test2.jl]] for the conventional approach. –>\n\nReference:  [1] Wang, B.Z., Hou, P.C., Deng, Y., Haule, K. and Chen, K., Fermionic sign structure of high-order Feynman diagrams in a many-fermion system. Physical Review B, 103, 115141 (2021).","category":"section"},{"location":"lib/mcmc/#Markov-chain-Monte-Carlo","page":"Markov-chain Monte Carlo","title":"Markov-chain Monte Carlo","text":"","category":"section"},{"location":"lib/mcmc/#MCIntegration.MCMC.montecarlo-Union{Tuple{T}, Tuple{O}, Tuple{P}, Tuple{V}, Tuple{N}, Tuple{Configuration{N, V, P, O, T}, Function, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Any, Any, Any}, Tuple{Configuration{N, V, P, O, T}, Function, Vararg{Any, 4}}} where {N, V, P, O, T}","page":"Markov-chain Monte Carlo","title":"MCIntegration.MCMC.montecarlo","text":"function montecarlo(config::Configuration{N,V,P,O,T}, integrand::Function, neval,\n    verbose=0, timer=[], debug=false;\n    measurefreq::Int=1, measure::Union{Nothing,Function}=nothing, idx::Int=1) where {N,V,P,O,T}\n\nThis algorithm computes high-dimensional integrals using a Markov-chain Monte Carlo (MCMC) method. It is particularly well-suited for cases involving multiple integrals over several variables. \n\nThe MCMC algorithm learns an optimal distribution, or 'ansatz', to best fit all integrands under consideration. Additionally, it introduces a normalization integral (with an integrand ~ 1) alongside the original integrals.\n\nAssume we have two integrals to compute, f_1(x) and f_2(x y), where x and y are variables of different types. The algorithm aims to learn the distributions rho_x(x) and rho_y(y), such that the quantities f_1(x)rho_x(x) and f_2(x y)rho_x(x)rho_y(y) are as flat as possible.\n\nUsing the Metropolis-Hastings algorithm, the algorithm samples variables x and y based on the joint distribution:\n\np(x y) = r_0 cdot rho_x(x) cdot rho_y(y) + r_1 cdot f_1(x) cdot rho_y(y) + r_2 cdot f_2(x y)\n\nwhere r_i are reweighting factors ensuring equal contribution from all terms. The integrals are then estimated by averaging the observables f_1(x)rho_y(y)p(x y) and f_2(x y)p(x y).\n\nSetting r_0 = 1 and r_i0 = 0 reduces this algorithm to the classic Vegas algorithm.\n\nThe key difference between this MCMC method and the :vegasmc solver lies in how the joint distribution p(x y) is sampled. This MCMC solver uses the Metropolis-Hastings algorithm to sample each term in p(x y) as well as the variables (x y). The MC configuration space thus consists of (idx, x, y), where idx represents the index of the user-defined and normalization integrals. In contrast, the :vegasmc algorithm only samples the (x, y) space, explicitly calculating all terms in p(x y) on-the-fly for a given set of x and y.\n\nNote: When multiple integrals are involved, only one of them is sampled and measured at each Markov-chain Monte Carlo step!\n\nWhile the MCMC method may be less efficient than the :vegasmc or:vegassolvers for low-dimensional integrations, it exhibits superior efficiency and robustness when faced with a large number of integrals, a scenario where the:vegasmcand:vegas` solvers tend to struggle.\n\nArguments\n\nintegrand: A user-defined function evaluating the integrand. The function should be integrand(idx, var, config). Here, idx is the index of the integrand component to be evaluated, var are the random variables and weights is an output array to store the calculated weights. The last parameter passes the MC Configuration struct to the integrand, so that user has access to userdata, etc.\nmeasure: An optional user-defined function to accumulate the integrand weights into the observable. The function signature should be measure(idx, var, obs, relative_weight, config). Here, idx is the integrand index, obs is a vector of observable values for each component of the integrand and relative_weight is the weight calculated from the idx-th integrand multiplied by the probability of the corresponding variables. \n\nThe following are the snippets of the integrand and measure functions:\n\nfunction integrand(idx, var, config)\n    # calculate your integrand values\n    # return integrand of the index idx\nend\n\nfunction measure(idx, var, obs, relative_weight, config)\n    # accumulates the weight into the observable\n    # For example,\n    # obs[idx] = relative_weight # integral idx\n    # ...\nend\n\nRemark:\n\nWhat if the integral result makes no sense?\nOne possible reason is the reweight factor. It is important for the Markov chain to visit the integrals with the similar frequency.  However, the weight of different integrals may be order-of-magnitude different. It is thus important to reweight the integrals.  Internally, the MC sampler try to reweight for each iteration. However, it could fail either 1) the total MC steps is too small so that  reweighting doesn't have enough time to show up; ii) the integrals are simply too different, and the internal reweighting subroutine is  not smart enough to figure out such difference. If 1) is the case, one either increase the neval. If 2) is the case, one may mannually  provide an array of reweight factors when initializes the MCIntegration.configuration struct. \n\nExamples\n\nThe following command calls the MC Vegas solver,\n\njulia> integrate((idx, x, c)->(x[1]^2+x[2]^2); var = Continuous(0.0, 1.0), dof = [[2,],], verbose=-1, solver=:mcmc)\nIntegral 1 = 0.6757665376867902 ± 0.008655534861083898   (reduced chi2 = 0.681)\n\n\n\n\n\n","category":"method"},{"location":"#MCIntegration","page":"Home","title":"MCIntegration","text":"Robust and efficient Monte Carlo calculator for high-dimensional integral.\n\n(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage)\n\nMCIntegration.jl is a robust and versatile Julia package designed to offer a comprehensive set of Monte Carlo integration algorithms. It's capable of calculating both regular and singular integrals across finite and infinite dimensions, which makes it a powerful tool in a wide array of scientific computing and data analysis contexts.\n\nHigh-dimensional integration is an inherently complex task often found in areas such as high-energy physics, material science, computational chemistry, financial mathematics, and machine learning. Traditional numerical integration techniques can falter when faced with the \"curse of dimensionality\". However, Monte Carlo methods, like those implemented in MCIntegration.jl, are particularly effective at overcoming this challenge.","category":"section"},{"location":"#Why-Julia?","page":"Home","title":"Why Julia?","text":"Julia combines the high-level simplicity and flexibility of Python with the performance capabilities of compiled languages like C/C++. This fusion makes it an ideal language for Monte Carlo integration.\n\nEfficiency is paramount in Monte Carlo methods due to the large number of computations. Julia, with its just-in-time (JIT) compilation, allows MCIntegration.jl to run these calculations with an efficiency close to that of lower-level languages like C/C++.\n\nOn the other hand, defining the integrands in Monte Carlo integration should be as easy and intuitive as possible. With Julia's high-level syntax, users can effortlessly define their own integrands, making MCIntegration.jl highly customizable and user-friendly.\n\nThis unique combination of performance and ease of use is what makes MCIntegration.jl, and Julia in general, stand out from other languages and tools.","category":"section"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"To help you get started with MCIntegration.jl, here are a few examples demonstrating its capabilities and usage.","category":"section"},{"location":"#Example-1.-One-dimensional-integral","page":"Home","title":"Example 1. One-dimensional integral","text":"We first show an example of highly singular integral. The following command evaluates ∫_0^1 log(x)/√x dx = -4.\n\njulia> res = integrate((x, c)->log(x[1])/sqrt(x[1]), solver=:vegas) \nIntegral 1 = -3.997980772652019 ± 0.0013607691354676158   (chi2/dof = 1.93)\n\njulia> report(res) #print out the iteration history\n====================================     Integral 1    ==========================================\n  iter              integral                            wgt average                  reduced chi2\n-------------------------------------------------------------------------------------------------\nignore        -3.8394711 ± 0.12101621              -3.8394711 ± 0.12101621                 0.0000\n     2         -3.889894 ± 0.04161423              -3.8394711 ± 0.12101621                 0.0000\n     3        -4.0258398 ± 0.016628525              -4.007122 ± 0.015441393                9.2027\n     4        -4.0010193 ± 0.0097242712            -4.0027523 ± 0.0082285382               4.6573\n     5         -3.990754 ± 0.0055248673            -3.9944823 ± 0.0045868638               3.5933\n     6         -4.000744 ± 0.0025751679            -3.9992433 ± 0.0022454867               3.0492\n     7        -4.0021542 ± 0.005940518             -3.9996072 ± 0.0021004392               2.4814\n     8        -3.9979708 ± 0.0034603885            -3.9991666 ± 0.0017955468               2.0951\n     9         -3.994137 ± 0.0026675679            -3.9975984 ± 0.0014895459               2.1453\n    10        -3.9999099 ± 0.0033455927            -3.9979808 ± 0.0013607691               1.9269\n-------------------------------------------------------------------------------------------------\n\nBy default, the function performs 10 iterations and each iteration costs about 1e4 evaluations. You can adjust these values using niter and neval keywords arguments.\nThe final result is obtained through an inverse-variance-weighted average of all iterations, excluding the first one (since there is no importance sampling yet!). The results are stored in res, which is a Result struct, and you can access the statistics with res.mean, res.stdev, res.chi2, and res.iterations.\nIf you want to exclude more iterations from the final estimations, such as the first three iterations, you can call Result(res, 3) to get a new averaged result.\nAfter each iteration, the program adjusts a distribution to mimic the integrand, improving importance sampling. Consequently, the estimated integral from each iteration generally becomes more accurate with more iterations. As long as neval is sufficiently large, the estimated integrals from different iterations should be statistically independent, justifying an average of different iterations weighted by the inverse variance. The assumption of statistical independence can be explicitly verified with a chi-square test, in which the chi2 (reduced chi^2) value should be approximately one.\nThe integrate function lets you choose a specific Monte Carlo (MC) algorithm by using the solver keyword argument. The example given employs the Vegas algorithm with :vegas. Additionally, this package provides two Markov-chain Monte Carlo (MCMC) algorithms for numerical integration: :vegasmc and :mcmc. Comparing these MCMC algorithms, :vegasmc offers better accuracy than :mcmc while keeping the same robustness. Although :vegas is generally slightly more accurate than :vegasmc, it is less robust. Considering the trade-off between accuracy and robustness, integrate defaults to using :vegasmc. For further information, consult the Algorithm section.\nWhen defining your own integrand evaluation function, you need to provide two arguments: (x, c):\nx represents the integration variable, which by default falls within the range [0, 1). It should be considered as a pool of infinitely many random variables that follows the same distribution. To access the i-th random variable, use x[i]. For a better understanding, refer to Example 2 and the Variables section.\nc is a struct that holds the Monte Carlo (MC) configuration. This contains additional information that might be necessary for evaluating the integrand. For a practical example, see Example 5.\nFor complex-valued integral, say with the type ComplexF64, you need to call integrate(..., dtype = ComplexF64) to specify the integrand data type. The error  of the real part and the imaginary part will be estimated independently.   \nYou can suppress the output information by setting verbose=-1. If you want to see more information after the calculation, simply call report(res). If you want to check the MC configuration, call report(res.config).","category":"section"},{"location":"#Example-2.-Multi-dimensional-integral:-Symmetric-Variables","page":"Home","title":"Example 2. Multi-dimensional integral: Symmetric Variables","text":"In MCIntegration.jl, a variable is represented as a pool of random numbers drawn from the same distribution. For instance, you can explicitly initialize a set of variables in the range [0, 1) as follows:\n\njulia> x=Continuous(0.0, 1.0) #Create a pool of continuous variables. \nAdaptive continuous variable in the domain [0.0, 1.0). Learning rate = 2.0.\n\nThis approach simplifies the evaluation of high-dimensional integrals involving multiple symmetric variables. For example, to calculate the area of a quarter unit circle (π/4 = 0.785398...):\n\njulia> res = integrate((x, c)->(x[1]^2+x[2]^2<1.0); var = x, dof = [2, ]) \nIntegral 1 = 0.7860119307731648 ± 0.002323473435947719   (reduced chi2 = 2.14)\n\nIf the integrand involves more than one variable, it is important to specify the dof vector. Each element of the dof vector represents the degrees of freedom of the corresponding integrand.","category":"section"},{"location":"#Example-3.-Multi-dimensional-integral:-Generic-Variables","page":"Home","title":"Example 3. Multi-dimensional integral: Generic Variables","text":"If the variables in a multi-dimensional integrand are not symmetric, it is better to define them as different types so that they can be sampled with different adaptive distributions. In the following example, we create a direct product of two continuous variables, then calculate a two-variable integral, \n\njulia> xy = Continuous([(0.0, 1.0), (0.0, 1.0)])\nAdaptive CompositeVar{Tuple{Continuous{Vector{Float64}}, Continuous{Vector{Float64}}}} with 2 components.\n\njulia> res = integrate(((x, y), c)-> log(x[1])/sqrt(x[1])*y[1]; var = xy)\nIntegral 1 = -2.0012850872834154 ± 0.001203058956026235   (reduced chi2 = 0.215)\n\nThe packed variable xy is of a type CompositeVar (see the Variables section.). It is unpacked into a tuple of x and y within the integrand function. ","category":"section"},{"location":"#Example-4.-Evaluate-Multiple-Integrands-Simultaneously","page":"Home","title":"Example 4. Evaluate Multiple Integrands Simultaneously","text":"You can calculate multiple integrals simultaneously. If the integrands are similar to each other, evaluating the integrals simultaneously sigificantly reduces the cost. The following example calculates the area of a quarter circle and one-eighth the volume of a sphere.\n\njulia> integrate((X, c)->(X[1]^2+X[2]^2<1.0, X[1]^2+X[2]^2+X[3]^2<1.0); var = Continuous(0.0, 1.0), dof = [[2,],[3,]])\nIntegral 1 = 0.7823432452235586 ± 0.003174967010742156   (reduced chi2 = 2.82)\nIntegral 2 = 0.5185515421806122 ± 0.003219487569949905   (reduced chi2 = 1.41)\n\nHere dof defines how many (degrees of freedom) variables there are of each type. For example, [[n1, n2], [m1, m2], ...] means the first integral involves n1 variables of type 1, and n2 variables of type2, while the second integral involves m1 variables of type 1 and m2 variables of type 2. As the dof of the integrals can be quite different, the program will figure out how to optimally pad the integrands to match the degrees of freedom.\n\nYou can also use the julia do-syntax to improve the readability of the above example,\n\njulia> integrate(var = Continuous(0.0, 1.0), dof = [[2,], [3,]]) do X, c\n           r1 = (X[1]^2 + X[2]^2 < 1.0) ? 1.0 : 0.0\n           r2 = (X[1]^2 + X[2]^2 + X[3]^2 < 1.0) ? 1.0 : 0.0\n           return (r1, r2)\n       end\n\nIf there are too many integrand components, it is better to preallocate the integrand weights. The function integrate provides an inplace key argument to achieve this goal. It is turned off by default, and only applies to the solvers :vegas and :vegasmc. Once inplace is turned on, integrate will call the user-defined integrand function with a preallocated vector to store the user-calculated weights. The following example demonstrates its usage,  \n\njulia> integrate(var = Continuous(0.0, 1.0), dof = [[2,], [3,]], inplace=true) do X, f, c\n           f[1] = (X[1]^2 + X[2]^2 < 1.0) ? 1.0 : 0.0\n           f[2] = (X[1]^2 + X[2]^2 + X[3]^2 < 1.0) ? 1.0 : 0.0\n       end","category":"section"},{"location":"#Example-5.-Use-Configuration-to-Interface-with-MCIntegration","page":"Home","title":"Example 5. Use Configuration to Interface with MCIntegration","text":"Configuration in integrands: As explained in the Example 1, the user-defined integrand has the signature (x, c) where x is the variable(s), and c is a 'Configuration' struct which stores the essential state information for the Monte Carlo sampling. Three particularly relevant members of Configuration include:\nuserdata : if you pass a keyword argument userdata to the integrate function then it will be stored here so that you can access it in your integrand evaluation function.\nvar : A tuple of variable(s). If there is only one variable in the tuple, then the first argument of the integrand will be x = var[1]. On the other hand, if there are multiple variables in the tuple, then x = var.\nobs : A vector of observables. Each element is an accumulated estimator for one integrand. In other words, length(obs) = length(dof) = number of integrands.\nnormalization: the integration estimate is given by obs ./ normalization.\nConfiguration in returned Result: The result returned by the integrate function contains the configuration after integration. If you want a detailed report, call report(res.config). This configuration stores the optimized random variable distributions for the importance sampling, which could be useful to evaluate other integrals with similar integrands. To use the optimized distributions, you can either call integrate(..., config = res.config, ...) to pass the entire configuration, or call integrate(..., var = (res.config.var[1], ...), ...) to pass one or more selected variables. In the following example, the second call is initialized with an optimized distribution, so that the first iteration is very accurate compared to the same row in the Example 1 output.\n\njulia> res0 = integrate((x, c)->log(x[1])/sqrt(x[1]))\nIntegral 1 = -3.999299273090788 ± 0.001430447199375744   (chi2/dof = 1.46)\n\njulia> res = integrate((x, c)->log(x[1])/sqrt(x[1]), verbose=0, config = res0.config)\n====================================     Integral 1    ================================================\n  iter              integral                            wgt average                      reduced chi2\n-------------------------------------------------------------------------------------------------------\nignore        -4.0022708 ± 0.0044299263            -4.0022708 ± 0.0044299263               0.0000\n     2        -3.9931774 ± 0.0042087902            -4.0022708 ± 0.0044299263               0.0000\n     3        -4.0003596 ± 0.0026421611            -3.9983293 ± 0.0022377558               2.0889\n     4        -3.9949943 ± 0.0027683518            -3.9970113 ± 0.0017402955               1.4833\n     5        -4.0028234 ± 0.0035948238            -3.9981148 ± 0.0015663954               1.6948\n     6        -4.0037708 ± 0.0021567542             -4.000068 ± 0.0012674021               2.3967\n     7        -3.9946345 ± 0.0040640646            -3.9995864 ± 0.0012099316               2.2431\n     8        -4.0039064 ± 0.0032909285            -4.0001008 ± 0.0011356123               2.1223\n     9        -3.9959395 ± 0.0036121885            -3.9997265 ± 0.0010833368               1.9916\n    10        -3.9955869 ± 0.0032874678             -3.999321 ± 0.0010289098               1.9215\n-------------------------------------------------------------------------------------------------------\nIntegral 1 = -3.9993209996786128 ± 0.0010289098118216647   (reduced chi2 = 1.92)","category":"section"},{"location":"#Example-6.-Measure-Histogram","page":"Home","title":"Example 6. Measure Histogram","text":"You may want to study how an integral changes with a tuning parameter. The following example is how to solve the histogram measurement problem.\n\njulia> N = 20;\n\njulia> grid = [i / N for i in 1:N];\n\njulia> function integrand(vars, config)\n            grid = config.userdata # radius\n            x, bin = vars #unpack the variables\n            r = grid[bin[1]] # binned variable in [0, 1)\n            r1 = x[1]^2 + r^2 < 1 # circle\n            r2 = x[1]^2 + x[2]^2 + r^2 < 1 # sphere\n            return r1, r2\n        end;\n\njulia> function measure(vars, obs, weights, config) \n       # obs: prototype of the observables for each integral\n           x, bin = vars #unpack the variables\n           obs[1][bin[1]] += weights[1] # circle\n           obs[2][bin[1]] += weights[2] # sphere\n       end;\n\njulia> res = integrate(integrand;\n                measure = measure, # measurement function\n                var = (Continuous(0.0, 1.0), Discrete(1, N)), # a continuous and a discrete variable pool \n                dof = [[1,1], [2,1]], \n                # integral-1: one continuous and one discrete variable, integral-2: two continous and one discrete variables\n                obs = [zeros(N), zeros(N)], # prototype of the observables for each integral\n                userdata = grid, neval = 1e5)\nIntegral 1 = 0.9957805541613277 ± 0.008336657854575344   (chi2/dof = 1.15)\nIntegral 2 = 0.7768105610812656 ± 0.006119386106596811   (chi2/dof = 1.4)\n\nYou can visualize the returned result res with Plots.jl. The commands res.mean[i] and res.stdev[i] give the mean and stdev of the histogram of the i-th integral.\n\njulia> using Plots\n\njulia> plt = plot(grid, res.mean[1], yerror = res.stdev[1], xlabel=\"R\", label=\"circle\", aspect_ratio=1.0, xlim=[0.0, 1.0])\n\njulia> plot!(plt, grid, res.mean[2], yerror = res.stdev[2], label=\"sphere\")\n\n(Image: histogram)","category":"section"},{"location":"#Algorithm","page":"Home","title":"Algorithm","text":"This package provides three solvers.\n\nVegas algorithm (:vegas): A Monte Carlo algorithm that uses importance sampling as a variance-reduction technique. Vegas iteratively builds up a piecewise constant weight function, represented on a rectangular grid. Each iteration consists of a sampling step followed by a refinement of the grid. The exact details of the algorithm can be found in G.P. Lepage, J. Comp. Phys. 27 (1978) 192, 3 and G.P. Lepage, Report CLNS-80/447, Cornell Univ., Ithaca, N.Y., 1980. \nVegas algorithm based on Markov-chain Monte Carlo (:vegasmc): A markov-chain Monte Carlo algorithm that uses the Vegas variance-reduction technique. It is as accurate as the vanilla Vegas algorithm, but tends to be more robust. For complicated high-dimensional integral, the vanilla Vegas algorithm can fail to learn the piecewise constant weight function. This algorithm uses the Metropolis–Hastings algorithm to sample the integrand and improve the weight function learning.\nMarkov-chain Monte Carlo (:mcmc): This algorithm is useful for calculating bundled integrands that are too numerous to calculate at once. Examples are the path-integrals of world lines of quantum particles, which involves hundreds and thousands of nested spacetime integrals. This algorithm uses the Metropolis-Hastings algorithm to jump between different integrals so that you only need to evaluate one integrand at each Monte Carlo step. Just as in :vegas and :vegasmc, this algorithm also learns a piecewise constant weight function to reduce the variance. However, because it assumes you can access only one integrand at each step, it tends to be less accurate than the other two algorithms for low-dimensional integrals.\n\nThe signature of the integrand and measure functions of the :mcmc solver receives an additional index argument compared to the :vegas and :vegasmc solvers. As shown in the above examples, the integrand and measure functions of the latter two solvers should look like integrand(vars, config) and measure(vars, obs, weights, config), where weights is a vector carrying the values of the integrands at the current MC step. On the other hand, the :mcmc solver requires something like integrand(idx, vars, config) and measure(idx, vars, weight, config), where idx is the index of the integrand of the current step, and the argument weight is a scalar carrying the value of the current integrand being sampled.","category":"section"},{"location":"#Variables","page":"Home","title":"Variables","text":"The package supports a couple of common types of random variables. You can create them using the following constructors:\n\nContinous(lower, upper[; adapt = true, alpha = 3.0, ...]): Continuous real-valued variables on the domain [lower, upper). MC will learn the distribution using the Vegas algorithm and then perform importance sampling accordingly.\nDiscrete(lower::Int, upper::Int[; adapt = true, alpha = 3.0, ...]): Integer variables in the closed set [lower, upper]. MC will learn the distribution and perform importance sampling accordingly.\n\nAfter each iteration, the code will try to optimize how the variables are sampled, so that the most important regimes of the integrals will be sampled most frequently. Setting alpha to be true/false will turn on/off this distribution learning. The parameter alpha controls the learning rate.\n\nWhen you call the above constructor, it creates an unlimited pool of random variables of a given type. The size of the pool will be dynamically determined when you call a solver. All variables in this pool will be sampled from the same distribution. In many high-dimensional integrals, many integration variables may contribute to the integral in a similar way; they can then be sampled from the same variable pool. For example, in the above code example, the integral for the circle area and the sphere volume both involve the variable type Continuous. The former has dof=2, while the latter has dof=3. When computing a multi-dimensional integrand, you only need to choose some of the variables to evaluate a given integral. The rest of the variables in the pool serve as dummy variables which will not cause any computational overhead.\n\nThe variable pool trick will significantly reduce the cost of learning their distribution. It also opens up the possibility of calculating integrals with infinite dimensions (for example, the path-integral of particle worldlines in quantum many-body physics).\n\nIf some of the variables are paired with each other (for example, the three continuous variables (r, θ, ϕ) representing a 3D vector), then you can pack them into a joint random variable, which can be constructed with the following constructor,\n\nCompositeVar(var1, var2, ...[; adapt = true, alpha = 3.0, ...]): A product of different types of random variables. It samples var1, var2, ... with the product of their distributions.\n\nIf the packed variables are all continuous or discrete, then you can create them in a more straightforward way,\n\nContinous([(lower1, upper1), (lower2, upper2), ...], [; adapt = true, alpha = 3.0, ...]).\nDiscrete([(lower1, upper1), (lower2, upper2), ...], [; adapt = true, alpha = 3.0, ...]).\n\nThe packed variables will all be sampled together in the Markov-chain based solvers (:vegasmc and :mcmc). Such updates will generate more independent samples compared to the unpacked version. This may reduce the auto-correlation time of the Markov chain and make the algorithm more efficient in some cases.\n\nMoreover, packed variables usually indicate nontrivial correlations between their distributions. In the future, it will be interesting to learn such correlations so that one can sample the packed variables more efficiently.\n\ntip: Integrate over different domains\nIf you want to compute an integral over a semi-infinite or an inifite domain, you can follow this advice from Steven G. Johnson: to compute an integral over a semi-infinite interval, you can perform the change of variables x=a+y(1-y):int_a^infty f(x)mathrmdx = int_0^1\nfleft(a + fracy1 - yright)frac1(1 - y)^2mathrmdyFor an infinite interval, you can perform the change of variables x=(2y - 1)((1 - y)y):int_-infty^infty f(x)mathrmdx = int_0^1\nfleft(frac2y - 1(1 - y)yright)frac2y^2 - 2y + 1(1 -\ny)^2y^2mathrmdyIn addition, recall that for an even function int_-infty^infty f(x)mathrmdx = 2int_0^inftyf(x)mathrmdx,  while the integral of an odd function over the infinite interval (-infty infty) is zero.","category":"section"},{"location":"#Parallelization","page":"Home","title":"Parallelization","text":"MCIntegration supports both MPI and multi-thread parallelization. You can even mix them if necessary.","category":"section"},{"location":"#MPI","page":"Home","title":"MPI","text":"To run your code in MPI mode, simply use the command,\n\nmpiexec -n #NCPU julia ./your_script.jl\n\nwhere #NCPU is the number of workers. Internally, the MC sampler will send the blocks (controlled by the argument Nblock, see above example code) to different workers, then collect the estimates in the root node.\n\nNote that you need to install the package MPI.jl to use the MPI mode. See this link for the instruction on the configuration.\n\nThe user essentially doesn't need to write additional code to support the parallelization. The only tricky part is the output: only the function MCIntegratoin.integrate of the root node returns meaningful estimates, while the other workers simply return nothing.","category":"section"},{"location":"#Multi-threading","page":"Home","title":"Multi-threading","text":"MCIntegration supports multi-threading with or without MPI. To run your code with multiple threads, start Julia with\n\njulia -t #NCPU ./your_script.jl\n\nNote that all threads will share the same memory. The user-defined integrand and measure functions should be implemented thread-safe (for example, be very careful about reading any data if another thread might write to it). We recommend the user read Julia's official documentation.\n\nThere are two different ways to parallelize your code with multiple threads.\n\nIf you need to evaluate multiple integrals, each thread can call the function MCIntegration.integrate to do one integral. In the following example, we use three threads to evaluate three integrals altogether. Note that only three threads will be used even if you initialize Julia with more than three threads.\n\njulia> Threads.@threads for i = 1:3\n       println(\"Thread $(Threads.threadid()) returns \", integrate((x, c) -> x[1]^i, print=-2))\n       end\nThread 2 returns Integral 1 = 0.24995156136254149 ± 6.945088534643841e-5   (chi2/dof = 2.95)\nThread 3 returns Integral 1 = 0.3334287563137184 ± 9.452648803649706e-5   (chi2/dof = 1.35)\nThread 1 returns Integral 1 = 0.5000251243601586 ± 0.00013482206569391864   (chi2/dof = 1.58)\n\nOnly the main thread calls the function MCIntegration.integrate, and then the internal blocks are parallelized across multiple threads. To do this, you need to call the function MCIntegration.integrate with keyword argument parallel = :thread. This approach will utilize all Julia threads. For example,\n\njulia> for i = 1:3\n       println(\"Thread $(Threads.threadid()) return \", integrate((x, c) -> x[1]^i, print=-2, parallel=:thread))\n       end\nThread 1 return Integral 1 = 0.5001880440214347 ± 0.00015058935731086765   (chi2/dof = 0.397)\nThread 1 return Integral 1 = 0.33341068551139696 ± 0.00010109649819894601   (chi2/dof = 1.94)\nThread 1 return Integral 1 = 0.24983868976137244 ± 8.546009018501706e-5   (chi2/dof = 1.54)","category":"section"}]
}
